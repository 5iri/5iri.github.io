{"0": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Understanding the Basics",
    "content": "The ApexCore project is an immersive journey into the world of computer architecture and organization (COA). Although the subject is vast and intricate, we, Shri Vishakh Devanand, Aditya Mahajan, and Shaurya Rane, began our exploration by understanding the core components of a CPU. We started with an edX course that, despite being taught in TLVerilog (a newer and less common programming language), provided us with the foundational knowledge necessary to progress. ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics"
  },"1": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "A simple breakdown of the CPU",
    "content": "To understand the logic of how it works, let’s go through the diagram with the flow. | Imagine a 32-bit instruction is going through to the CPU. | Instructions are stored in Instruction Memory and they are accessed via addresses stored in Program Counter. The instruction received by the decoder to decode the instruction into what generally occurs into 3 or 4 different things. The instruction type is defined by the operation code. | The source values are given as the address of its place in the register, rather than the value itself, or for some instructions there comes something called immediate values, where you don’t want to store the value but just use the value for another calculation and lose it. | Then the destination register, where the calculated value is going to be stored until it is stored in memory or discarded. | Then, it goes into the control unit, where it decides what type of calculation is required for this particular instruction and sends it to the ALU to calculate it. | . | Once the final result is received by the control unit, it is then stored in the destination register as instructed. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu"
  },"2": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "What have we done so far",
    "content": "We have completed testing RV32-IM extensions under simulation, which introduces new instructions for multiplication and division. Here’s a detailed breakdown of our progress: . | Code Optimization: We revisited the M extension to refine the code, focusing on enhancing efficiency and ensuring proper functionality. | Simulation: Using Vivado, we simulated the RV32-IM extensions. The simulation verified that the new instructions for multiplication and division were correctly implemented and functioning as expected. | Debugging and Validation: During simulation, we meticulously debugged the code to identify and fix any issues. This process was crucial for validating the correctness of our implementation. | Documentation: Alongside coding and simulation, we documented our process and findings. This documentation will serve as a valuable reference for future development and troubleshooting. | Preparation for Hardware Testing: We have prepared the code for the upcoming hardware testing phase. This includes ensuring compatibility with our target FPGA platform and setting up the necessary testing environment. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far"
  },"3": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Plans for the coming days",
    "content": ". | We are about to test the current code on hardware and ensure it works with high confidence. | Keep adding more extensions like Atomic and Floating-point extensions to further increase the general usage of the core. | Further End Goal of the project to create a soft-core that can be used for other projects, who don’t need to build a cpu from scratch in order to do so. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days"
  },"4": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "ApexCore A RISC-V based CPU",
    "content": "–Shri Vishakh Devanand –Aditya Mahajan –Shaurya Rane . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html"
  },"5": {
    "doc": "What is Abstract Algebra?",
    "title": "What is Abstract Algebra?",
    "content": "# What is Abstract Algebra? ## Introduction to Groups - **Abstract Algebra** is the study of algebraic structures that generalize the arithmetic of numbers. - The famous mathematician **Évariste Galois** laid the foundation for Abstract Algebra in the 19th century when he discovered solutions for higher-degree polynomials using groups. - At the same time, **Carl Friedrich Gauss** formalized the concept of modular arithmetic, which is a key concept in Abstract Algebra. --- ## Technical terms - **Element:** An object in a set. - **Set:** A collection of objects. - **Binary Operation:** A rule that combines two elements of a set to produce another element. - **Identity element:** An element that leaves other elements unchanged when combined with them. $$e$$ is the identity element if $$a * e = e * a = a$$ for all $$a$$ in the set. - **Inverse Element:** For each element $$a$$ in the set, there exists an element $$a^{-1}$$ such that $$a * a^{-1} = a^{-1} * a = e$$. - **Order of a Group:** The number of elements in a group. (Represented as $$ \\|G\\|$$) --- ## Definition of a Group - A **Group** is a set $$G$$ with a binary operation $$*$$ that satisfies the following properties: - **Closure:** For all $$a, b \\in G $$, the element $$a * b$$ is also in $$G$$. ($$x,y \\in G \\Rightarrow x * y \\in G$$) - **Associativity:** For all $$a, b, c \\in G$$, $$ (a * b) * c = a * (b * c) $$ - **Identity Element:** There exists an element $$e \\in G$$ such that for all $$a \\in G$$, $$ a * e = e * a = a $$ - **Inverse Element:** For each $a \\in G$, there exists an element $$a^{-1} \\in G$$ such that $$ a * a^{-1} = a^{-1} * a = e $$ - Although this is the formal definition, the concept of a group can be understood more intuitively through examples. - e.g., The set of integers under addition forms a group. - e.g., The set of rotations or flips of a triangle forms a group. - $$G$$ may _not_ be commutative $$\\Rightarrow x * y \\neq y * x $$ - A group is called **Abelian** if it is commutative. - A group is called **Non-Abelian** if it is not commutative. --- ## What is a Subgroup? - Assume a group $$G$$ with a binary operation $$*$$. - A subset $$H$$ of $$G$$ is a **subgroup** of $$G$$ if it is itself a group under the operation $$*$$. - $$ H \\leq G $$ denotes that $$H$$ is a subgroup of $$G$$. - If $$H$$ is a proper subgroup of $$G$$, we write $$H 2 $$ is non-abelian. **note:** every finite group is a subgroup of a symmetric group. This is called Cayley's theorem. ### Cycle notation of permutations This is a method to rewrite the permutation that you get from Symmetric groups in less than $$n$$ elements. **Simple Example: ** - Consider a Symmetric Group of $$S_4$$ - this can be written as (as an example) $$\\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 2 & 1 & 4 & 3 \\end{pmatrix} $$ This is one permutation of $$S_4$$, like this there are many (specifically $$4!$$) let's take another permutation, $$\\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 3 & 4 & 1 & 2 \\end{pmatrix} $$ We can clearly see that these can be converted into different functions, f(1) = 2; f(2) = 1; f(3) = 4; f(4) = 3; similarly g(1) = 3; g(2) = 4; g(3) = 1; g(4) = 2; where the numbers on top is the input, and the numbers on the bottom as the output. Remember, each permutation is an element in the symmetric group. $$\\Rightarrow$$ we can see that the group operation has to be like a function composition. $$ f \\circ g $$ So, $$ f \\circ g $$ = $$\\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 4 & 3 & 2 & 1 \\end{pmatrix} $$ But man, this is too lengthy to keep writing for different larger and larger examples. So, We introduce Cycle notations. - They are basically shorthand for writing this weird matrix. - In the above example, I can write $$f$$ and $$g$$ as follows: $$1 \\to 2 \\to 1 $$ and $$3 \\to 4 \\to 3 $$ So, $$f$$ = $$(1, 2)(3, 4)$$ and $$g$$ = $$(1,3)(2,4)$$ This makes it a whole lotta easy to read and write because now, you can see that 1 and 2 are interconnected and 3 and 4 are interconnected in $$f$$ and from there it's easy to build the permutation. **Sidenote: ** It's neat to note that the order of the cyclic decomposition does not matter and the order in which the cycles have been written doesn't matter either. $$(1,2)(4,3)$$ = $$(3,4)(1,2)$$ Two element cycles are called a **transposition**. It's important to note that we haven't done any group operation on the cyclic decomposition yet. (the brackety ones just above). So, to do that, we need to start continue with this example. f = $$(1,2)(3,4)$$ g = $$(1,3)(2,4)$$ Cycles with no numbers in common can be rearranged, whereas cycles with common numbers in them have to keep it the same. first take 1, 1 $$\\to$$ 3 $$\\to $$ 4 then take 2, 2 $$\\to$$ 4 $$\\to$$ 1 like this you do and you get this, $$ f \\circ g $$ = $$(4, 3, 2, 1)$$ You can see that you get the same answer here! The technical definition of rearrangement Cycles with no numbers in common \"commute\" ### Dihedral Group - The word Dihedral means \"two faces\" - There are three different operations that an $$n$$ sided regular polygon can do - rotation (defined as $$r$$) - flip (defined as $$f$$) - No change (aka identity element $$e$$) For this let's look at the symmetry groups of the equilateral triangle. Here, the no change in rotation or flipping will be the identity element $$e$$. When you rotate the triangle by $$120^\\circ$$, then it can be defined as $$r$$. When you rotate the triangle by $$240^\\circ$$, then it can be defined as rotating twice, so it can be defined as $$r^2$$. When you rotate the triangle by $$360^\\circ$$, you can see that it comes back to the same position, hence we can say that $$r^3 = e$$. you can also flip the triangle vertically, which can be defined as $$f$$. you can also see that you flip the triangle twice, it brings you back to the same place. So, $$f^2 = e $$. So the total transformations that can be done which is distinct is as follows: - $$e, r, r^2, f ,r\\cdot f ,r^2 \\cdot f $$ - Also note, $$\\|r\\|$$ = 3 and $$\\|f\\|$$ = 2. Similarly, you can see the same thing for isosceles and scalene triangle. (I'm too lazy to work it out, but its so simple tbh) ### Matrix Groups - Groups made using matrices are called the Matrix groups. - There are two operations that can be done under matrices. $$ \\times$$ or $$+$$. - Identity element under addition for an $$ n \\ \\times \\ m $$ matrix is just 0s in all elements. (which is represented by $$O$$). - For Matrices under Multiplication, for the group to be abelian and infinite, We take the only $$n \\times n$$ matrices, with $$I$$ as the identity matrix. It should also be noted that for a matrix to have inverses, det($$M$$) $$\\neq$$ 0. - The general linear group, $$GL_n(R)$$ = $$n \\times n$$ invertible matrices. - The Special Linear group, $$SL_n(R)$$ = $$n \\times n$$ invertible matrices with det($$A$$) must be equal to 1. and $$A \\ \\epsilon \\ SL_n(R)$$ - Note that the $$R$$ here, denotes that the elements in the matrix are real. ### Direct products of Groups - Like how prime numbers are the building blocks of integers (as in any integer can be decomposed into the product of primes), Groups can be decomposed by their building blocks, $$Simple \\ groups$$. (TBD) ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html"
  },"6": {
    "doc": "Multi-Agent Path Finding",
    "title": "Multi-Agent Path Finding",
    "content": "# Multi-Agent Path Finding --- ## Introduction - MAPF is a planning problem in which the task is to plan paths for multiple agents, where the key constraint is that the agents will be able to follow these paths while not colliding with each other. - This problem has a range of research groups and academic communities which have studied them but they all have very different terminoligies used, and someof them have also different objectives in their paper. - The paper aims to address this issue by introducing unified terminology to describe MAPF problems. - The second part of the paper introduces a new grid MAPF benchmark. --- ## Classical MAPF #### Description of the classical MAPF - The input to a classical MAPF problem with k agents is a tuple , where G = (V, E) [Have some doubts here(1. What is V and E here? V can be assumed to be Vector but I have no idea what E is.) ] is an undirected graph, s : [1,.....,k] -> V maps an agent to a source vertex. t : [1,......,k] -> V maps an agent to a target vertex. - Time is assumed to be discretized, and in every step each agent is situated in one of the graph vertices and can perform a single *action*. - An *Action* in classical MAPF is a function *a* : *V* -> *V* such that *a(v) = v'* means if an agent is at vertex *v* and performs an action, then it will be in vertex *v'* in the next time step. - So Each agent has two types of actions : *wait* and *move*. - *wait* means that the agent stays exactly where it is. - *move* means the agents moves to another adjacent block (or *vertex* as written in the paper). - For a sequence of actions *represented as &pi;* = $$(a_{1}, ..... a_{n})$$, and an agent *i*, we denote by $$\\pi_{i} \\left[ x \\right]$$ = $$a_{x}(a_{x-1}(...a_{1}(s(i)))$$. - $$\\pi$$ is a **single-agent plan** for agent $$i$$ iff executing this sequence of actions in $$s(i)$$ results in being at $$t(i)$$, $$ \\pi_{i}[|\\pi |] = t(i) $$ - A solution is a set of $k$ single-agent plans, one for each agent. This means we could ig say it forms a $k * n$ matrix?? --- #### Types of Conflicts in Classical MAPF - There is a possibility of $$vertex \\ conflict$$ between $$\\pi_{i}$$ and $$\\pi_j$$ occuring when both agents plans to occupy a particular positions. - There is a possibility of $$edge\\ conflict$$ between $$\\pi_{i}$$ and $$\\pi_{j}$$ occuring iff where both the agents plan to traverse to the same edge at the same time and step in the same direction. - Formally, there is an edge conflict between $$\\pi_{i}$$ and $$\\pi_{j}$$ iff there exists a time step $x$ such that $$\\pi_{i} [x] = \\pi_{j} [x]$$ and $$\\pi_{i}[x+1] = \\pi_{j} [ x+1]$$. /// Don't know when is this possible, in the sense we already have vertex conflict which basically says that $$\\pi_{i} = \\pi_{j}$$ right? - $$Following\\ conflict$$ is between $$\\pi_{i}$$ and $$\\pi_{j}$$ occurs iff one agent is planned to occupy a vertex that was occupied by another agent in the previous time step. Now, that's gonna be ignored since occupied positions are something that we are neglecting in here. // Why is this a big issue? - $$\\pi_{i} [x+1] = \\pi_{j} [x]$$ mathematically written. - A conflict where multiple agents can enter into a position forming a loop like | x | x | x | x | x | x | x | x | x | x |---|---|---|---|---|---|---|---|---|---| x | 1 | 2 | x | x | x | x | x | x | x | x | 3 | 4 | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | -- This is called as $$Cycle\\ conflict$$. - $$π_{i}(x + 1) = π_{i+1}(x)$$ - $$Swapping\\ conflict$$ is like the cycle one, where it would just switch the vertices. - Cosidering the formal definitions of these conflicts, it is clear that there are dominance in relation with them, 1. Forbidding vertex conflicts also implies edge conflicts are forbidden. 2. Forbidding following conflicts also implies cycle conflicts are also forbidden. 3. Forbidding cycle conflicts implies that swapping conflicts are also forbidden. - This is also true vice versa. - To properly define a classical MAPF problem, one needs to specify which types of conflicts are allowed in a solution. - The least constrained restriction is to only forbid edge conflicts. (In our task, ig Swapping and edge are not allowed so only 1 and 3 are forbidden per say). --- #### Agent Behaviour at Target in Classical MAPF - There are two ccommon assumptions that are done when defining a classical MAPF problem, - **Stay at target** : Under this assumption, an agent waits in its target untill all agents have reached their targets. This will then cause a vertex conflict with any plan that passes through its target after it has reached it. - **Disappear at target**: Under this assumption, when an agent reaches its target it immediately disappears. This means the plan of that agent will not have any conflict after the time step in which the corresponding agent has reached its target. --- ### Objective Functions in Classical MAPF - To capture the best solution, we consider something called as **objective functions** that is used to evaluate MAPF solutions. The two most common functions used for evaluating a solution in classical MAPF are $$makespan$$ and $$sum\\ of\\ costs$$. - $$Makespan$$ : The number of time steps required for all agents to reach their target. - $$Sum\\ of \\ costs$$: The sum of time steps required by each agent to reach its target. It is also called as *flowtime*. - For example, assume that agent i reaches its target at time step t, leaves its target at time step t ' , arrives back at its target at time step t'', and then stays at its target until all agents reach their target. Then, this single-agent plan will contribute t'' to the sum of costs of the corresponding solution. --- ### Beyond Classical MAPF - As of now, the assumptions taken are: 1. time is discretized into time steps, 2. every action takes exactly one time step, 3. in every time step, each agent occupies exactly a single vertex. --- ### MAPF on Weighted Graphs - The notation $$G$$ now is a weighted graph where the weight of each edge represents the duration it will take an agent to traverse this edge. - Types of graphs that have been included in this paper: 1. MAPF in $$2^k$$-neighbor grids -- These maps are restricted form of weighted graphs in which every vertex represents a cell in a two dimensional grid. **DID NOT UNDERSTAND THIS** 2. MAPF in Euclidean space -- These maps are a generalization of MAPF in which every node in $$G$$ represents a Euclidean point ($$x$$, $$y$$), and the edges represent allowed move actions. Such settings arise when the underlying graph is a roadmap generated for a continuous Euclidean environment. **ABSOLUTELY DID NOT UNDERSTAND THIS EITHER**. --- ### Feasibility Rules - Depending on what you want to lose and what you want in priority, there are sufficient rules for the classical MAPF solution. - **Robustness rules** -- These rules are designed to ensure that a MAPF solution considers inadvertant delays in execution. A $$k-robust$$ MAPF plan builds in a sufficient buffer for agents to be delayed up to $$k$$ time steps without resulting in a conflict. This is only when the probability of future delays are known (ig this is where we can use RL if I am not wrong, where we can use ML to predict the probability of a task in a delay. - **Formation rules** -- this is restriction added whenever there is some action that needs to be decided by the bot (or \"$$agent$$\" as given in the paper). --- ### From Pathfinding to Motion planning - We are now considering bots now not in vertex but with something as a limited speed and a variety of sizes ($$configuration$$ is a better word). - **MAPF with large agents** -- This basically now introduces volume to the vertex, and now a single $$agent$$ can take up multiple vertices. Hence, it may prevent some other bot to move or even take use of those vertices. It may also not allow other $$agents$$ along the edge whenever one $$agent$$ is along the edge. - * There are several approaches to solving these kinds of issues, including a CBS-based approach and a prioritized planning approach. A special case of agents with volume is the convoy setting, in which agents occupy a string of vertices and their connecting edges. - **MAPF with kinematic constraints** -- This basically is now introducing another parameter which needs to also think about how to move over to another location without falling or whatever it is not to do to reach the final destination. - A by-product of such contraints is that the underlying graph becomes directed, as there may be edges that can only be passable in one-direction due to kinematic constraints of the agent. - There is some reduction-based approach that assumes rotation actions as a half way to kinematic constraints. --- ### Tasks and Agents - In classical MAPF, one agent --- one task -- to get it to its target. Several extensions have been made in the MAPF literature in which agents may be assigned more than one target. - **Anonymous MAPF** --- The objective is to move the agents to a set of target vertices, but it does not matter which agent reaches which target. (Like, in a place where all the products are the same, it doesn't matter which person goes and takes the box to be delivered, all we need to know is that *that* thing is delivered on time. - **Colored MAPF** --- The objective here is to generalize and group the agents into teams, and each team has their own sets of targets. **(THIS IS THE TASK WE ARE GOING TO WORK ON)**. - Another way to view this variant, is as a MAPF problem in which every agent can be assigned to targets only from the set of targets designated for its team. - This can be generalised even further, assigning a target and an agent to multiple teams. - **Online MAPF** --- In $$online$$ MAPF, a sequence of MAPF problems are solved on the same graph. This setting has also been called \"Lifelong MAPF\". Online MAPF problems can be classified as follow. - **warehouse model** -- This is the setting where a fixed set of agents, solve a MAPF problem, but after an agent finds a target, it may be tasked to go to different target. This setting is inspired by MAPF for autonomous warehouses. **(This is also another way to represent the task since, we do have fixed number of agents for each team, and they are always given some task \"$target$\", which once is done, it is made to go to another location to do another task, which is also what is being done, so basically we are doing a mix of colored and Online MAPF).** - **Interesection model** -- This is the setting where new agents may appear and each agent has one task -- to reach its target. This setting is inspired by autonomous vehicles entering and exiting intersections. ",
    "url": "/markdown_files/posts/content/MAPF.html",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html"
  },"7": {
    "doc": "Notes on how are cancer cells grow and spread through the body",
    "title": "Notes on how are cancer cells grow and spread through the body",
    "content": "# Notes on how are cancer cells grow and spread through the body --- ## What are cancer cells? - fuck ups in DNA causes genes to mutate, which makes a particular types of cells to not listen to the conditions for which the genes to mutate. ## How does Cancer grow? - A cancer cell is different from normal ceels because they: - divide out of control - are immature and don't develop into doing their jobs - avoid the immune system. - ignore signals - don't stick to each other very well and can spread to other parts of the body through the blood or lymphatic system - grow into and damage tissues and organs ## How cancer spreads? - Normal spread usually by pushing on normal tissues and growing into a big ball in the same place, this is called local invasion or invasive cancer. - Cancer can also spread from where it first started to other parts of the body. This process is called metastatis. Cancer cells can metastatsize when they break away from the tumour and travel to a new location in the body through the blood or lymphatic system. ## Why does cancer come back sometimes? - even if 1 cancer cell is left begind during cancer treatment, it has the possibility of growing and dividing into a new tumour. - In some cases, treatment may stop working (become resistant) so cancer cells are no longer being destroyed. ## Why use acoustic tweezers? - It produces high and low acoustic pressures, enabling the movement or positioning of target cells with precision. This allows them to be used for manipulating metastasis-related cancer cells. - The flow cytometry results prove that it is non-invasive. - It can be used to isolate circulating tumor cells (CTCs), and enables in developing targeted treatments. ## how to make acoustic tweezers? - The new device works by placing sound-creating transducers on each side of a small square chamber filled with liquid. These four transducers work in step with those directly across from them, forming two pairs. One creates patterns in the chamber horizontally and the other vertically. The interaction of these two complex, quickly changing sound wave patterns creates dynamic abilities never before demonstrated within the field. ## what is acoustic streaming? - Acoustic streaming is another important principle that underlies acoustic tweezers. When a sound wave passes through a fluid medium, it creates tiny vortexes of fluid around the edges of the wave. These vortexes can be used to move small objects within the fluid, even against the direction of the sound wave itself. ## Use cases other than biomedicine - In microelectronics, acoustic tweezers have the potential to revolutionize the manufacturing process for microchips and other tiny electronic components. By using sound waves to position and manipulate these components, researchers can improve the accuracy and efficiency of the manufacturing process. - ## resources - [How cancer starts, grows and spreads](https://cancer.ca/en/cancer-information/what-is-cancer/how-cancer-starts-grows-and-spreads) - [acoustic tweezers](https://acoustofluidics.pratt.duke.edu/research/acoustic-tweezers) - [Cells Dancing to Harmonic Duets Could Enable Personalized Cancer Therapies](https://pratt.duke.edu/news/harmonic-acoustic-tweezers/) ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html"
  },"8": {
    "doc": "Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution",
    "title": "Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution",
    "content": "# Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution ``` by yale patt ``` ## Basic Framework - Computer Architecture: A science of tradeoffs - Comp arch is more \"art\" than \"science\" - Almost always the job of the comp architect requires using that fundamental knowledge to make tradeoffs. - Levels of Transformation - There are levels of transformations that is being done whenever there's a high level program that is being done. - Like take for example a C code, it first compiles into an assembly code (a lot more process that happens but not imp here). - This \"assembly\" code is basically converted into where ISA structure is used, where ISA defines on how the assembly code is being defined. - Now, Even though the \"assembly\" code is now converted into something in binary, how this binary is being processed is (which bits need to go where in order for the \"assembly\" line to be processed actually) [Instruction is being processed] - This is called microarchitecture. - This microarchitecture is defined by actual electronic circuit design, and in the end electrons. - These are the levels of transformation that occurs from any problem to actually making it solve by a circuit (or by electrons). ![Levels of Transformation](/markdown_files/posts/assets/comparchreads/image1.png) - Design Points - When working in the microarchitecture part, there is always a purpose towards what you are trying to achieve. - This is called the Design Point. - It could be something like, making a core more power efficient , fault tolerant (server chips), highest power etc. - Application Space - Sometimes the chips that we design are application specific, and sometimes generalistic in nature, this is what we call application space here. - The Basics of Processing - Simply put, a microprocessor processes instructions. - To do this, it has to do three things: - Supply instructions to the core of the processor where the instructions are executed. - Supply data required for the instructions - Perform the operation required by each instruction. - Instruction Supply - fetching one instruction at a time is slow when processors are getting faster and better at processing them. - So, the fetching of instructions should be done in parallel, making it more efficient. - Data Supply - Once the instruction is fetched, the data required for the instruction is supplied to the processor. - This must be done in a fast manner as well. - So faster caches (on-chip RAM) are used to supply the data for a particular instruction. - Instruction Processing - Once the instruction is fetched and the data is supplied, the instruction must also be processed fast. - To perform the operations required by these instructions, the processor needs a sufficient number of functional units to process the data. ## Agents for Evolution - The creativity of engineers to come up with answers where there were problems--- without solutions, there would be no Evolution. - Agent I: New Requirements - The demand for higher performance dictated that fetching one instruction at a time was not enough. - Examples like this forces evolution and creative solution thinking. - Agent II: Bottlenecks - We have 3 components of instruction processing (instruction supply, data supply, and carrying out the operations of the instruction). - By far, most of the improvements to the microprocessor have come about due to attempts to eliminate bottlenecks that prevent these components from being fully utilized. - Agent III: Good Fortune - Good Fortune happens when something causes a windfall which can then be used to provide additional features to the microprocessor. # References - [Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution](https://course.ece.cmu.edu/~ece740/f13/lib/exe/fetch.php?media=r0_patt.pdf) ",
    "url": "/markdown_files/posts/content/comparchreads1.html",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html"
  },"9": {
    "doc": "Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems",
    "title": "Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems",
    "content": "# Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems **Thomas Moscibroda** **Onur Mutlu** @microsoft ## Introduction - The transition from single-core to multi-core systems has introduced major performance and security challenges. - Multiple programs running on shared DRAM systems can interfere with each other's memory accesses, leading to performance degradation and security vulnerabilities. - This paper introduces a new security problem that arises due to the core design of multi-core architectures – a denial of service (DoS) attack that was not possible in single-core systems. - An aggressive memory-intensive program can severely impact the performance of other threads with which it is co-scheduled. This is called a Memory Performance Hog (MPH). - This problem worsens with an increasing number of cores, as the impact grows exponentially. - An MPH can be used to perform DoS attacks that fool users into thinking other applications are inherently slow, even without causing easily observable performance issues. - A regular application can unintentionally behave like an MPH and damage the memory-related performance of co-scheduled threads. ## DRAM Architectures - DRAM memory is an expensive resource in modern systems. Creating a separate DRAM system for each core is not feasible. - In a partitioned DRAM system, a processor accessing a memory location needs to issue a request to the DRAM partition that contains the data for that location. ## DRAM Memory Systems ![DRAM BANK ORGANIZATION](/markdown_files/posts/assets/DRAM_Block_diagram.png) - **Row Hit:** Accessing a row already in the row-buffer. It has the lowest latency (around 40-50 ns in commodity DRAM). - **Row Conflict:** Accessing a different row than the one currently in the row-buffer, requiring the row-buffer to be written back before the new row can be accessed. - **Row Closed:** No row in the row-buffer, necessitating a read from the memory array before column access. ## DRAM Controller - The DRAM controller mediates between on-chip caches and off-chip DRAM memory. It receives read/write requests from L2 caches. - The **memory access scheduler** is responsible for selecting memory requests from the memory request buffer to send to the DRAM memory. ## Memory Access Scheduling Algorithm - Current memory access schedulers typically employ the **First-Ready First-Come-First-Serve (FR-FCFS)** algorithm, which prioritizes requests in the following order: - **Row-hit first:** Prioritizes requests that hit the row already in the row-buffer. - **Oldest-within-bank first:** Prioritizes requests that arrived earliest within the same bank. - **Oldest-across-banks first:** Prioritizes the earliest arrival time among requests selected by individual bank schedulers. ## Vulnerabilities of Multi-Core DRAM Memory System to DoS Attacks - Current DRAM memory systems do not distinguish between the requests of different threads. - **Unfairness of Row-Hit First Scheduling:** A thread whose accesses result in row hits gets higher priority compared to a thread whose accesses result in row conflicts. - **Unfairness of Oldest-First Scheduling:** Oldest-first scheduling implicitly favors threads that can generate memory requests at a faster rate than others. ## Examples of DoS in Existing Multi-Cores - When two threads use different access patterns, such as one streaming data and the other accessing memory randomly, the memory controller will prioritize the one with the optimized memory access pattern. ## Fairness in DRAM Memory Systems - Defining fairness in DRAM systems is complex, and coming up with a reasonable definition is challenging. ## Fair Memory Scheduling: A Model - The authors propose a model for fair memory scheduling to mitigate the impact of MPHs. - **Fairness Definition:** A memory scheduler is fair if equal-priority threads experience the same memory-related slowdowns when running together. - **Stall-Time Fair Memory Scheduler (STFM):** STFM prioritizes threads based on their stall times, ensuring that no thread monopolizes memory resources, thus promoting fairness. - **Implementation Considerations:** STFM requires modifications to the memory controller to track stall times for each thread, ensuring equitable memory access for co-scheduled threads. ## Conclusion - The paper highlights the vulnerabilities of multi-core systems to DoS attacks due to unfair memory access scheduling. - By introducing the concept of Memory Performance Hogs and the Stall-Time Fair Memory Scheduler, the authors offer a framework to enhance fairness and improve both the performance and security of multi-core systems. ## References - The paper is written by Professor Onur Mutlu which can be found [here](https://users.ece.cmu.edu/~omutlu/pub/mph_usenix_security07.pdf) ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html"
  },"10": {
    "doc": "Yo!",
    "title": "Yo!",
    "content": "# Yo! I'm **Shri Vishakh Devanand**, a sophomore engineering student from India absolutely obsessed with mathematics, physics and electronics. I thrive at the intersection of hardware and software, applying complex mathematical concepts to solve real engineering challenges. ## What I Do - Built a fully functional RISC-V CPU on FPGA from scratch - Dive deep into complex analysis, abstract algebra, and discrete mathematics - Developing custom zig based compiler for ESP32. - Create bio-inspired robots that mimic animal behavior. ## My Vision Mathematics is my universal language for innovation - whether optimizing algorithms, designing circuits, or modeling control systems. I'm constantly exploring the beautiful intersection where theoretical math meets practical engineering. ## Fun Facts - Maintain a personal notebook of elegant mathematical proofs (collecting all of them into a single one is remaining 😅) - Can lose myself for hours in puzzles - Love to challenge myself Do check out my Github and LinkedIn profiles to know more about my projects and interests. Feel free to reach out to me for collaborations or discussions on any of the topics you are passionate about. I'm always up for a good chat! Also check out my blogs on the navigation bar! 😄 ",
    "url": "/",
    
    "relUrl": "/"
  }
}
