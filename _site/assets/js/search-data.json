{"0": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Understanding the Basics",
    "content": "The ApexCore project is an immersive journey into the world of computer architecture and organization (COA). Although the subject is vast and intricate, we, Shri Vishakh Devanand, Aditya Mahajan, and Shaurya Rane, began our exploration by understanding the core components of a CPU. We started with an edX course that, despite being taught in TLVerilog (a newer and less common programming language), provided us with the foundational knowledge necessary to progress. ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics"
  },"1": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "A simple breakdown of the CPU",
    "content": "To understand the logic of how it works, let’s go through the diagram with the flow. | Imagine a 32-bit instruction is going through to the CPU. | Instructions are stored in Instruction Memory and they are accessed via addresses stored in Program Counter. The instruction received by the decoder to decode the instruction into what generally occurs into 3 or 4 different things. The instruction type is defined by the operation code. | The source values are given as the address of its place in the register, rather than the value itself, or for some instructions there comes something called immediate values, where you don’t want to store the value but just use the value for another calculation and lose it. | Then the destination register, where the calculated value is going to be stored until it is stored in memory or discarded. | Then, it goes into the control unit, where it decides what type of calculation is required for this particular instruction and sends it to the ALU to calculate it. | . | Once the final result is received by the control unit, it is then stored in the destination register as instructed. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu"
  },"2": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "What have we done so far",
    "content": "We have completed testing RV32-IM extensions under simulation, which introduces new instructions for multiplication and division. Here’s a detailed breakdown of our progress: . | Code Optimization: We revisited the M extension to refine the code, focusing on enhancing efficiency and ensuring proper functionality. | Simulation: Using Vivado, we simulated the RV32-IM extensions. The simulation verified that the new instructions for multiplication and division were correctly implemented and functioning as expected. | Debugging and Validation: During simulation, we meticulously debugged the code to identify and fix any issues. This process was crucial for validating the correctness of our implementation. | Documentation: Alongside coding and simulation, we documented our process and findings. This documentation will serve as a valuable reference for future development and troubleshooting. | Preparation for Hardware Testing: We have prepared the code for the upcoming hardware testing phase. This includes ensuring compatibility with our target FPGA platform and setting up the necessary testing environment. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far"
  },"3": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Plans for the coming days",
    "content": ". | We are about to test the current code on hardware and ensure it works with high confidence. | Keep adding more extensions like Atomic and Floating-point extensions to further increase the general usage of the core. | Further End Goal of the project to create a soft-core that can be used for other projects, who don’t need to build a cpu from scratch in order to do so. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days"
  },"4": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "ApexCore A RISC-V based CPU",
    "content": "–Shri Vishakh Devanand –Aditya Mahajan –Shaurya Rane . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html"
  },"5": {
    "doc": "What is Abstract Algebra?",
    "title": "What is Abstract Algebra?",
    "content": "# What is Abstract Algebra? Abstract Algebra is the fascinating study of algebraic structures that generalize the arithmetic of numbers. It's a field that revolutionized mathematics in the 19th century and continues to be fundamental to many branches of mathematics today. --- ## Introduction to Groups The roots of Abstract Algebra can be traced back to the brilliant minds of the 19th century. The famous mathematician **Évariste Galois** laid its foundation when he discovered solutions for higher-degree polynomials using groups. Simultaneously, **Carl Friedrich Gauss** was formalizing the concept of modular arithmetic, which would become a cornerstone in Abstract Algebra. --- ## Technical Terms Before we dive deeper, let's understand some essential terminology: - **Element:** An object in a set. - **Set:** A collection of objects. - **Binary Operation:** A rule that combines two elements of a set to produce another element. - **Identity element:** An element that leaves others unchanged when combined with them. For example, $$e$$ is the identity element if $$a * e = e * a = a$$ for all $$a$$ in the set. - **Inverse Element:** For each element $$a$$ in the set, there exists an element $$a^{-1}$$ such that $$a * a^{-1} = a^{-1} * a = e$$. - **Order of a Group:** The number of elements in a group, represented as $$ \\|G\\|$$. --- ## Definition of a Group At the heart of Abstract Algebra is the concept of a **Group**. A Group is a set $$G$$ with a binary operation $$*$$ that satisfies four fundamental properties: 1. **Closure:** For all $$a, b \\in G $$, the element $$a * b$$ is also in $$G$$. ($$x,y \\in G \\Rightarrow x * y \\in G$$) 2. **Associativity:** For all $$a, b, c \\in G$$, $$ (a * b) * c = a * (b * c) $$ 3. **Identity Element:** There exists an element $$e \\in G$$ such that for all $$a \\in G$$, $$ a * e = e * a = a $$ 4. **Inverse Element:** For each $a \\in G$, there exists an element $$a^{-1} \\in G$$ such that $$ a * a^{-1} = a^{-1} * a = e $$ While this formal definition might seem abstract, the concept becomes more intuitive through examples: - The set of integers under addition forms a group. - The set of rotations or flips of a triangle forms a group. It's important to note that a group $$G$$ may not be commutative, meaning $$x * y \\neq y * x$$. A group that is commutative is called **Abelian**, and one that isn't is called **Non-Abelian**. --- ## What is a Subgroup? If we have a group $$G$$ with a binary operation $$*$$, a subset $$H$$ of $$G$$ is a **subgroup** of $$G$$ if it is itself a group under the operation $$*$$. We denote this as $$ H \\leq G $$, meaning $$H$$ is a subgroup of $$G$$. If $$H$$ is a proper subgroup of $$G$$ (meaning $$H \\neq G$$), we write $$H The order of a subgroup divides the order of the group. Mathematically: If $$H \\leq G \\Rightarrow \\|G\\| \\ \\text{divides} \\ \\|H\\|$$ It's important to note that the converse of Lagrange's Theorem is not true. #### Proof of Lagrange's Theorem Let's prove this beautiful theorem step by step: Assume $$G$$ is a finite group with $$\\|G\\| = n$$. **Case 1:** Let $$\\{e\\} \\leq G$$ and $$\\|{e}\\| = 1$$. $$\\|G\\| = n = 1 \\times n$$, which is true. **Case 2:** Let $$\\|G\\| = n$$ and $$G \\leq G$$. $$\\|G\\| = n = n \\times 1$$, which is true. **Case 3:** Let $$\\|H\\| = k $$ and $$H 2$$ are non-abelian. An important result in group theory is **Cayley's Theorem**, which states that every finite group is isomorphic to a subgroup of a symmetric group. --- ## Cycle notation of permutations This is a method to represent permutations in a more concise form. **Simple Example:** Consider the symmetric group $$S_4$$. A permutation in this group can be written as: $$\\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 2 & 1 & 4 & 3 \\end{pmatrix}$$ This means: - $$1$$ maps to $$2$$ - $$2$$ maps to $$1$$ - $$3$$ maps to $$4$$ - $$4$$ maps to $$3$$ Similarly, another permutation might be: $$\\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 3 & 4 & 1 & 2 \\end{pmatrix}$$ These can be viewed as functions. If we call the first permutation $$f$$ and the second $$g$$, then: - $$f(1) = 2$$, $$f(2) = 1$$, $$f(3) = 4$$, $$f(4) = 3$$ - $$g(1) = 3$$, $$g(2) = 4$$, $$g(3) = 1$$, $$g(4) = 2$$ The group operation is function composition, $$f \\circ g$$, which gives: $$f \\circ g = \\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 4 & 3 & 2 & 1 \\end{pmatrix}$$ But writing permutations in this matrix form can be cumbersome, especially for larger groups. That's where cycle notation comes in. In cycle notation, we write: - $$f = (1,2)(3,4)$$ - $$g = (1,3)(2,4)$$ This makes it much easier to read and write permutations. The notation $(a, b, c, ...)$ means $$a$$ maps to $$b$$, which maps to $$c$$, and so on, with the last element mapping back to $$a$$. **Sidenote:** The order of the cycles in a permutation doesn't matter, and the order of elements within a cycle (except for the starting point) doesn't matter either. So $(1,2)(4,3) = (3,4)(1,2)$. Two-element cycles, like $(1,2)$, are called **transpositions**. --- ## Dihedral Group The word **Dihedral** means \"two faces.\" These groups describe the symmetries of regular polygons. There are three types of operations a regular $$n$$-sided polygon can undergo: 1. Rotation (denoted as $$r$$) 2. Flip (denoted as $$f$$) 3. No change (the identity element $$e$$) For an equilateral triangle: - No change in position is the identity element $$e$$. - Rotating by $$120^\\circ$$ is denoted as $$r$$. - Rotating by $$240^\\circ$$ can be denoted as $$r^2$$ (rotating twice). - Rotating by $$360^\\circ$$ brings us back to the starting position, so $$r^3 = e$$. - Flipping the triangle vertically is denoted as $$f$$. - Flipping twice returns to the original position, so $$f^2 = e$$. The total distinct transformations for the triangle are: $$e, r, r^2, f, r \\cdot f, r^2 \\cdot f$$ Also, note that the order of $$r$$ is 3 ($$\\|r\\| = 3$$) and the order of $$f$$ is 2 ($$\\|f\\| = 2$$). --- ## Matrix Groups Groups formed using matrices are called **Matrix Groups**. The two common operations with matrices are addition and multiplication. Under addition, the identity element for an $$n \\times m$$ matrix is a matrix with all elements as 0 (denoted as $$O$$). For matrices under multiplication, the group is typically limited to $$n \\times n$$ matrices. The identity element is the identity matrix $$I$$. For a matrix to have an inverse (and hence be part of a group under multiplication), its determinant must be non-zero, i.e., det($$M$$) $$\\neq$$ 0. Some important matrix groups include: - **General Linear Group** ($$GL_n(R)$$): The group of $$n \\times n$$ invertible matrices. - **Special Linear Group** ($$SL_n(R)$$): The group of $$n \\times n$$ invertible matrices with determinant 1. The $$R$$ indicates that the matrix elements are real numbers. --- ## Direct products of Groups Just as integers can be decomposed into products of primes, groups can be broken down into simpler groups. One way to build complex groups from simpler ones is through direct products. Given two groups $$G_1$$ and $$G_2$$, their direct product is defined as: $$G_1 \\times G_2 = \\{(x,y) \\mid x \\in G_1, y \\in G_2\\}$$ The direct product of multiple groups is also possible: $$G_1 \\times G_2 \\times G_3 \\times \\ldots \\times G_n = \\{(x_1, x_2, x_3, \\ldots, x_n) \\mid x_1 \\in G_1, x_2 \\in G_2, \\ldots, x_n \\in G_n\\}$$ The order of the direct product is the product of the orders of its constituent groups: $$\\|G_1 \\times G_2 \\times \\ldots \\times G_n\\| = \\|G_1\\| \\times \\|G_2\\| \\times \\ldots \\times \\|G_n\\|$$ The identity element of the direct product is the tuple of identity elements from each group: $$(e_1, e_2, e_3, \\ldots, e_n)$$, where $$e_i$$ is the identity element of $$G_i$$. If any of the groups in the direct product is non-abelian, then the direct product itself is also non-abelian. --- ## Simple Groups (In Detail) A subgroup $$N$$ of a group $$G$$ is **normal** if $$g \\cdot N \\cdot g^{-1} = N$$ for all $$g \\in G$$. A group $$G$$ is **simple** if its only normal subgroups are {1} and $$G$$ itself. Let's consider a finite group $$G$$ with a normal subgroup $$N_1 \\triangleleft G$$. We say $$N_1$$ is **maximal** and **proper** if: **Maximal**: There's no group $$H$$ between $$G$$ and $$N_1$$, i.e., no group $$H$$ with $$N_1 \\subset H \\subset G$$. **Proper**: $$N_1 \\neq G$$. We can continue this process, finding a normal subgroup of $$N_1$$, and so on, forming a **normal series**: $$1 \\triangleleft N_i \\triangleleft N_{i-1} \\triangleleft \\ldots \\triangleleft N_2 \\triangleleft N_1 \\triangleleft G$$ When this series is made as long as possible, it's called a **composition series**. ### Jordan-Hölder Theorem The **Jordan-Hölder Theorem** states that if there are multiple composition series for a finite group $$G$$, they are equivalent in the sense that they have the same length and identical factor groups. ### Classification of Finite Simple Groups In a composition series, each quotient group is a simple group. This means that understanding all finite groups can be broken down into two tasks: 1. Find all simple groups. 2. Find all extensions of simple groups. This leads to the **extension problem**: Given a finite group $$N$$ and a simple group $$S$$, find all groups $$G$$ where $$N \\triangleleft G$$ and $$G/N \\cong S$$. Researchers have made significant progress in classifying all finite simple groups. There are four categories: 1. **Cyclic groups of prime order**: $$Z/pZ$$ where $$p$$ is prime. These are simple because their only divisors are 1 and $$p$$, which means their only subgroups are {0} and the group itself. 2. **Alternating groups** $$A_n$$ for $$n \\geq 5$$. These relate to the non-existence of general formulas for solving polynomial equations of degree 5 or higher. 3. **Groups of Lie Type**: These are related to manifolds, spaces that locally resemble Euclidean space. One example is the group of complex numbers with absolute value 1, which forms a circle. Another is the group of real, invertible $$n \\times n$$ matrices. 4. **26 Sporadic Groups**: These are exceptional groups that don't fit into the other categories. Among them, the **Monster Group** is the largest, with approximately $$8 \\times 10^{53}$$ elements. The Monster contains 20 of the 26 sporadic groups, known as the \"happy family,\" while the remaining 6 are called the \"pariahs.\" The classification of finite simple groups is one of the monumental achievements of modern mathematics, spanning approximately 10,000 pages of research. --- ## The Definition of a Ring A **Ring** is a set of elements with operations similar to addition and multiplication, though the inverses of these operations may not always be defined. For example, a $$2 \\times 2$$ matrix with real elements forms a ring. A ring is always \"closed,\" meaning the result of any operation on ring elements is also in the ring. Technically, a ring is a set $$R$$ with two operations, addition ($$+$$) and multiplication ($$\\times$$), satisfying: - Closure: For $$x, y \\in R$$, both $$x + y \\in R$$ and $$x \\times y \\in R$$. - The set forms a commutative group under addition. - Multiplication is associative. - Multiplication is distributive over addition. If a ring is commutative under multiplication, it's called a **commutative ring**. If a ring contains a multiplicative identity (usually denoted as 1), it's called a **ring with identity**. --- ## Examples of Rings ### 1. The Set of Integers ($$Z$$) The set of all positive and negative whole numbers, including zero, forms a ring under the operations of addition and multiplication. **Properties**: - **Closure**: Adding or multiplying two integers gives another integer. - **Associativity**: Both addition and multiplication are associative. - **Additive Identity**: 0 serves as the additive identity. - **Multiplicative Identity**: 1 serves as the multiplicative identity. - **Additive Inverses**: For every integer $$a$$, there's an integer $$-a$$ such that $$a + (-a) = 0$$. - **Distributivity**: Multiplication distributes over addition. ### 2. The Set of Polynomials with Real Coefficients ($$R[x]$$) This is the set of all polynomials where the coefficients are real numbers. **Properties**: - **Closure**: The sum and product of two polynomials in $$R[x]$$ are also in $$R[x]$$. - **Associativity**: Both addition and multiplication are associative. - **Additive Identity**: The zero polynomial is the additive identity. - **Multiplicative Identity**: The polynomial 1 is the multiplicative identity. - **Additive Inverses**: For every polynomial $$p(x)$$, there's a polynomial $$-p(x)$$ such that $$p(x) + (-p(x)) = 0$$. - **Distributivity**: Multiplication distributes over addition. ### 3. The Set of $$n \\times n$$ Matrices with Real Entries ($$M_n(R)$$) This is the set of all $$n \\times n$$ matrices where each entry is a real number. **Properties**: - **Closure**: The sum and product of two $$n \\times n$$ matrices are also $$n \\times n$$ matrices. - **Associativity**: Matrix addition and multiplication are associative. - **Additive Identity**: The zero matrix is the additive identity. - **Multiplicative Identity**: The identity matrix is the multiplicative identity. - **Additive Inverses**: For every matrix $$A$$, there's a matrix $$-A$$ such that $$A + (-A) = 0$$. - **Distributivity**: Matrix multiplication distributes over addition. ### 4. The Set of Continuous Functions from $$R$$ to $$R$$ ($$C(R, R)$$) This is the set of all continuous functions mapping real numbers to real numbers. **Properties**: - **Closure**: The sum and product (pointwise) of two continuous functions are continuous. - **Associativity**: Function addition and multiplication are associative. - **Additive Identity**: The zero function ($$f(x) = 0$$ for all $$x$$) is the additive identity. - **Multiplicative Identity**: The constant function $$f(x) = 1$$ is the multiplicative identity. - **Additive Inverses**: For every function $$f$$, there's a function $$-f$$ such that $$f + (-f) = 0$$. - **Distributivity**: Function multiplication distributes over addition. --- ## Units in a Ring ### Definition of Units A **unit** in a ring is an element that has a multiplicative inverse. If $$a$$ is an element in a ring $$R$$ and there exists a $$b \\in R$$ such that $$a \\times b = 1$$, then $$a$$ is a unit, and $$b$$ is its inverse. ### Group of Units The set of all units in a ring forms a **group under multiplication**, known as the **group of units**. ### Examples - In the ring of integers ($$Z$$), the only units are 1 and -1, as these are the only integers with multiplicative inverses within $$Z$$. - In the ring of real numbers ($$R$$), every non-zero real number is a unit, since each has a multiplicative inverse. --- ## Integral Domains An **Integral Domain** is a commutative ring $$R$$ with a multiplicative identity and no zero divisors. This means that if $$a \\times b = 0$$, then either $$a = 0$$ or $$b = 0$$. A key property of integral domains is that they satisfy the **cancellation law**: If $$a \\times b = a \\times c$$ and $$a \\neq 0$$, then $$b = c$$. --- ## Ideals in Rings An **Ideal** is a subset of a ring that is closed under addition and multiplication by elements of the ring. It's analogous to how subgroups relate to groups. Formally, if $$I$$ is an ideal of a ring $$R$$, then: - For all $$a, b \\in I$$, $$a + b \\in I$$ (closed under addition) - For all $$a \\in I$$ and $$r \\in R$$, $$a \\times r \\in I$$ and $$r \\times a \\in I$$ (closed under multiplication by ring elements) --- ## Key Concepts | Concept | Description |-------------------|----------------------------------------------------------| Normal Subgroup | Subgroup N of G, forms factor group G/N. | Ring | Abelian group under addition, associative multiplication.| Ideal | Additive subgroup I of R, r·i, i·r ∈ I. | Factor Ring | Cosets of R modulo I form ring R/I. | - Left Ideals and Right Ideals also exist and are important in non-commutative rings. - The intersection of two ideals is also an ideal. - The sum of two ideals is also an ideal. - The product of two ideals is also an ideal. --- ## What are Fields? A **Field** is a ring where all non-zero elements have multiplicative inverses. This means it can perform all the operations familiar from arithmetic. Formally, a field is a set $$F$$ with two operations, addition and multiplication, such that: - The set forms a commutative group under addition. - The set of non-zero elements forms a commutative group under multiplication. - Multiplication is distributive over addition. ### Prime Fields The smallest fields are called **prime fields**. These are fields of integers modulo a prime number, like $$Z/2Z$$, $$Z/3Z$$, $$Z/5Z$$, and so on. The **characteristic of a field** is the smallest positive integer $$n$$ such that $$n \\times 1 = 0$$. If no such $$n$$ exists, the field has characteristic 0. --- ## Vector Spaces A **Vector Space** is a set of elements (vectors) along with operations of vector addition and scalar multiplication. The scalars come from a field. A vector space over a field $$F$$ must satisfy several axioms, including: - The set of vectors forms a commutative group under addition. - Scalar multiplication is distributive over vector addition. - Scalar multiplication is compatible with field multiplication. - The multiplicative identity of the field acts as a scalar that leaves vectors unchanged. --- ## Modules A **Module** generalizes the concept of a vector space by replacing the field of scalars with a ring. This allows for a broader study of algebraic structures. Formally, a module over a ring $$R$$ consists of a set of elements (like vectors) together with operations of addition and scalar multiplication by elements of $$R$$, satisfying axioms similar to those of a vector space. The key difference between a module and a vector space is that modules allow for scalar multiplication from a ring, while vector spaces require scalars from a field. --- ## References - [A YouTube playlist by Socratica](https://www.youtube.com/playlist?list=PLi01XoE8jYoi3SgnnGorR_XOW3IcK-TP6) - [A Book of Abstract Algebra - Charles C. Pinter](https://math.umd.edu/~jcohen/402/Pinter%20Algebra.pdf) - [Abstract Algebra - Theory and Applications - Judson, Stephen](https://math.mit.edu/~mckernan/Teaching/12-13/Spring/18.703/book.pdf) --- I hope this guide has provided a comprehensive introduction to the beautiful world of Abstract Algebra. From the foundational concept of groups to the more advanced topics of rings, fields, and modules, Abstract Algebra offers a fascinating glimpse into the deeper structures that underlie mathematics. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html"
  },"6": {
    "doc": "Multi-Agent Path Finding",
    "title": "Multi-Agent Path Finding",
    "content": "# Multi-Agent Path Finding --- ## Introduction - MAPF is a planning problem in which the task is to plan paths for multiple agents, where the key constraint is that the agents will be able to follow these paths while not colliding with each other. - This problem has a range of research groups and academic communities which have studied them but they all have very different terminoligies used, and someof them have also different objectives in their paper. - The paper aims to address this issue by introducing unified terminology to describe MAPF problems. - The second part of the paper introduces a new grid MAPF benchmark. --- ## Classical MAPF #### Description of the classical MAPF - The input to a classical MAPF problem with k agents is a tuple , where G = (V, E) [Have some doubts here(1. What is V and E here? V can be assumed to be Vector but I have no idea what E is.) ] is an undirected graph, s : [1,.....,k] -> V maps an agent to a source vertex. t : [1,......,k] -> V maps an agent to a target vertex. - Time is assumed to be discretized, and in every step each agent is situated in one of the graph vertices and can perform a single *action*. - An *Action* in classical MAPF is a function *a* : *V* -> *V* such that *a(v) = v'* means if an agent is at vertex *v* and performs an action, then it will be in vertex *v'* in the next time step. - So Each agent has two types of actions : *wait* and *move*. - *wait* means that the agent stays exactly where it is. - *move* means the agents moves to another adjacent block (or *vertex* as written in the paper). - For a sequence of actions *represented as &pi;* = $$(a_{1}, ..... a_{n})$$, and an agent *i*, we denote by $$\\pi_{i} \\left[ x \\right]$$ = $$a_{x}(a_{x-1}(...a_{1}(s(i)))$$. - $$\\pi$$ is a **single-agent plan** for agent $$i$$ iff executing this sequence of actions in $$s(i)$$ results in being at $$t(i)$$, $$ \\pi_{i}[|\\pi |] = t(i) $$ - A solution is a set of $k$ single-agent plans, one for each agent. This means we could ig say it forms a $k * n$ matrix?? --- #### Types of Conflicts in Classical MAPF - There is a possibility of $$vertex \\ conflict$$ between $$\\pi_{i}$$ and $$\\pi_j$$ occuring when both agents plans to occupy a particular positions. - There is a possibility of $$edge\\ conflict$$ between $$\\pi_{i}$$ and $$\\pi_{j}$$ occuring iff where both the agents plan to traverse to the same edge at the same time and step in the same direction. - Formally, there is an edge conflict between $$\\pi_{i}$$ and $$\\pi_{j}$$ iff there exists a time step $x$ such that $$\\pi_{i} [x] = \\pi_{j} [x]$$ and $$\\pi_{i}[x+1] = \\pi_{j} [ x+1]$$. /// Don't know when is this possible, in the sense we already have vertex conflict which basically says that $$\\pi_{i} = \\pi_{j}$$ right? - $$Following\\ conflict$$ is between $$\\pi_{i}$$ and $$\\pi_{j}$$ occurs iff one agent is planned to occupy a vertex that was occupied by another agent in the previous time step. Now, that's gonna be ignored since occupied positions are something that we are neglecting in here. // Why is this a big issue? - $$\\pi_{i} [x+1] = \\pi_{j} [x]$$ mathematically written. - A conflict where multiple agents can enter into a position forming a loop like | x | x | x | x | x | x | x | x | x | x |---|---|---|---|---|---|---|---|---|---| x | 1 | 2 | x | x | x | x | x | x | x | x | 3 | 4 | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | x | -- This is called as $$Cycle\\ conflict$$. - $$π_{i}(x + 1) = π_{i+1}(x)$$ - $$Swapping\\ conflict$$ is like the cycle one, where it would just switch the vertices. - Cosidering the formal definitions of these conflicts, it is clear that there are dominance in relation with them, 1. Forbidding vertex conflicts also implies edge conflicts are forbidden. 2. Forbidding following conflicts also implies cycle conflicts are also forbidden. 3. Forbidding cycle conflicts implies that swapping conflicts are also forbidden. - This is also true vice versa. - To properly define a classical MAPF problem, one needs to specify which types of conflicts are allowed in a solution. - The least constrained restriction is to only forbid edge conflicts. (In our task, ig Swapping and edge are not allowed so only 1 and 3 are forbidden per say). --- #### Agent Behaviour at Target in Classical MAPF - There are two ccommon assumptions that are done when defining a classical MAPF problem, - **Stay at target** : Under this assumption, an agent waits in its target untill all agents have reached their targets. This will then cause a vertex conflict with any plan that passes through its target after it has reached it. - **Disappear at target**: Under this assumption, when an agent reaches its target it immediately disappears. This means the plan of that agent will not have any conflict after the time step in which the corresponding agent has reached its target. --- ### Objective Functions in Classical MAPF - To capture the best solution, we consider something called as **objective functions** that is used to evaluate MAPF solutions. The two most common functions used for evaluating a solution in classical MAPF are $$makespan$$ and $$sum\\ of\\ costs$$. - $$Makespan$$ : The number of time steps required for all agents to reach their target. - $$Sum\\ of \\ costs$$: The sum of time steps required by each agent to reach its target. It is also called as *flowtime*. - For example, assume that agent i reaches its target at time step t, leaves its target at time step t ' , arrives back at its target at time step t'', and then stays at its target until all agents reach their target. Then, this single-agent plan will contribute t'' to the sum of costs of the corresponding solution. --- ### Beyond Classical MAPF - As of now, the assumptions taken are: 1. time is discretized into time steps, 2. every action takes exactly one time step, 3. in every time step, each agent occupies exactly a single vertex. --- ### MAPF on Weighted Graphs - The notation $$G$$ now is a weighted graph where the weight of each edge represents the duration it will take an agent to traverse this edge. - Types of graphs that have been included in this paper: 1. MAPF in $$2^k$$-neighbor grids -- These maps are restricted form of weighted graphs in which every vertex represents a cell in a two dimensional grid. **DID NOT UNDERSTAND THIS** 2. MAPF in Euclidean space -- These maps are a generalization of MAPF in which every node in $$G$$ represents a Euclidean point ($$x$$, $$y$$), and the edges represent allowed move actions. Such settings arise when the underlying graph is a roadmap generated for a continuous Euclidean environment. **ABSOLUTELY DID NOT UNDERSTAND THIS EITHER**. --- ### Feasibility Rules - Depending on what you want to lose and what you want in priority, there are sufficient rules for the classical MAPF solution. - **Robustness rules** -- These rules are designed to ensure that a MAPF solution considers inadvertant delays in execution. A $$k-robust$$ MAPF plan builds in a sufficient buffer for agents to be delayed up to $$k$$ time steps without resulting in a conflict. This is only when the probability of future delays are known (ig this is where we can use RL if I am not wrong, where we can use ML to predict the probability of a task in a delay. - **Formation rules** -- this is restriction added whenever there is some action that needs to be decided by the bot (or \"$$agent$$\" as given in the paper). --- ### From Pathfinding to Motion planning - We are now considering bots now not in vertex but with something as a limited speed and a variety of sizes ($$configuration$$ is a better word). - **MAPF with large agents** -- This basically now introduces volume to the vertex, and now a single $$agent$$ can take up multiple vertices. Hence, it may prevent some other bot to move or even take use of those vertices. It may also not allow other $$agents$$ along the edge whenever one $$agent$$ is along the edge. - * There are several approaches to solving these kinds of issues, including a CBS-based approach and a prioritized planning approach. A special case of agents with volume is the convoy setting, in which agents occupy a string of vertices and their connecting edges. - **MAPF with kinematic constraints** -- This basically is now introducing another parameter which needs to also think about how to move over to another location without falling or whatever it is not to do to reach the final destination. - A by-product of such contraints is that the underlying graph becomes directed, as there may be edges that can only be passable in one-direction due to kinematic constraints of the agent. - There is some reduction-based approach that assumes rotation actions as a half way to kinematic constraints. --- ### Tasks and Agents - In classical MAPF, one agent --- one task -- to get it to its target. Several extensions have been made in the MAPF literature in which agents may be assigned more than one target. - **Anonymous MAPF** --- The objective is to move the agents to a set of target vertices, but it does not matter which agent reaches which target. (Like, in a place where all the products are the same, it doesn't matter which person goes and takes the box to be delivered, all we need to know is that *that* thing is delivered on time. - **Colored MAPF** --- The objective here is to generalize and group the agents into teams, and each team has their own sets of targets. **(THIS IS THE TASK WE ARE GOING TO WORK ON)**. - Another way to view this variant, is as a MAPF problem in which every agent can be assigned to targets only from the set of targets designated for its team. - This can be generalised even further, assigning a target and an agent to multiple teams. - **Online MAPF** --- In $$online$$ MAPF, a sequence of MAPF problems are solved on the same graph. This setting has also been called \"Lifelong MAPF\". Online MAPF problems can be classified as follow. - **warehouse model** -- This is the setting where a fixed set of agents, solve a MAPF problem, but after an agent finds a target, it may be tasked to go to different target. This setting is inspired by MAPF for autonomous warehouses. **(This is also another way to represent the task since, we do have fixed number of agents for each team, and they are always given some task \"$target$\", which once is done, it is made to go to another location to do another task, which is also what is being done, so basically we are doing a mix of colored and Online MAPF).** - **Interesection model** -- This is the setting where new agents may appear and each agent has one task -- to reach its target. This setting is inspired by autonomous vehicles entering and exiting intersections. ",
    "url": "/markdown_files/posts/content/MAPF.html",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html"
  },"7": {
    "doc": "Notes on how are cancer cells grow and spread through the body",
    "title": "Notes on how are cancer cells grow and spread through the body",
    "content": "# Notes on how are cancer cells grow and spread through the body --- ## What are cancer cells? - fuck ups in DNA causes genes to mutate, which makes a particular types of cells to not listen to the conditions for which the genes to mutate. ## How does Cancer grow? - A cancer cell is different from normal ceels because they: - divide out of control - are immature and don't develop into doing their jobs - avoid the immune system. - ignore signals - don't stick to each other very well and can spread to other parts of the body through the blood or lymphatic system - grow into and damage tissues and organs ## How cancer spreads? - Normal spread usually by pushing on normal tissues and growing into a big ball in the same place, this is called local invasion or invasive cancer. - Cancer can also spread from where it first started to other parts of the body. This process is called metastatis. Cancer cells can metastatsize when they break away from the tumour and travel to a new location in the body through the blood or lymphatic system. ## Why does cancer come back sometimes? - even if 1 cancer cell is left begind during cancer treatment, it has the possibility of growing and dividing into a new tumour. - In some cases, treatment may stop working (become resistant) so cancer cells are no longer being destroyed. ## Why use acoustic tweezers? - It produces high and low acoustic pressures, enabling the movement or positioning of target cells with precision. This allows them to be used for manipulating metastasis-related cancer cells. - The flow cytometry results prove that it is non-invasive. - It can be used to isolate circulating tumor cells (CTCs), and enables in developing targeted treatments. ## how to make acoustic tweezers? - The new device works by placing sound-creating transducers on each side of a small square chamber filled with liquid. These four transducers work in step with those directly across from them, forming two pairs. One creates patterns in the chamber horizontally and the other vertically. The interaction of these two complex, quickly changing sound wave patterns creates dynamic abilities never before demonstrated within the field. ## what is acoustic streaming? - Acoustic streaming is another important principle that underlies acoustic tweezers. When a sound wave passes through a fluid medium, it creates tiny vortexes of fluid around the edges of the wave. These vortexes can be used to move small objects within the fluid, even against the direction of the sound wave itself. ## Use cases other than biomedicine - In microelectronics, acoustic tweezers have the potential to revolutionize the manufacturing process for microchips and other tiny electronic components. By using sound waves to position and manipulate these components, researchers can improve the accuracy and efficiency of the manufacturing process. - ## resources - [How cancer starts, grows and spreads](https://cancer.ca/en/cancer-information/what-is-cancer/how-cancer-starts-grows-and-spreads) - [acoustic tweezers](https://acoustofluidics.pratt.duke.edu/research/acoustic-tweezers) - [Cells Dancing to Harmonic Duets Could Enable Personalized Cancer Therapies](https://pratt.duke.edu/news/harmonic-acoustic-tweezers/) ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html"
  },"8": {
    "doc": "Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution",
    "title": "Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution",
    "content": "# Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution ``` by yale patt ``` ## Basic Framework - Computer Architecture: A science of tradeoffs - Comp arch is more \"art\" than \"science\" - Almost always the job of the comp architect requires using that fundamental knowledge to make tradeoffs. - Levels of Transformation - There are levels of transformations that is being done whenever there's a high level program that is being done. - Like take for example a C code, it first compiles into an assembly code (a lot more process that happens but not imp here). - This \"assembly\" code is basically converted into where ISA structure is used, where ISA defines on how the assembly code is being defined. - Now, Even though the \"assembly\" code is now converted into something in binary, how this binary is being processed is (which bits need to go where in order for the \"assembly\" line to be processed actually) [Instruction is being processed] - This is called microarchitecture. - This microarchitecture is defined by actual electronic circuit design, and in the end electrons. - These are the levels of transformation that occurs from any problem to actually making it solve by a circuit (or by electrons). ![Levels of Transformation](/markdown_files/posts/assets/comparchreads/image1.png) - Design Points - When working in the microarchitecture part, there is always a purpose towards what you are trying to achieve. - This is called the Design Point. - It could be something like, making a core more power efficient , fault tolerant (server chips), highest power etc. - Application Space - Sometimes the chips that we design are application specific, and sometimes generalistic in nature, this is what we call application space here. - The Basics of Processing - Simply put, a microprocessor processes instructions. - To do this, it has to do three things: - Supply instructions to the core of the processor where the instructions are executed. - Supply data required for the instructions - Perform the operation required by each instruction. - Instruction Supply - fetching one instruction at a time is slow when processors are getting faster and better at processing them. - So, the fetching of instructions should be done in parallel, making it more efficient. - Data Supply - Once the instruction is fetched, the data required for the instruction is supplied to the processor. - This must be done in a fast manner as well. - So faster caches (on-chip RAM) are used to supply the data for a particular instruction. - Instruction Processing - Once the instruction is fetched and the data is supplied, the instruction must also be processed fast. - To perform the operations required by these instructions, the processor needs a sufficient number of functional units to process the data. ## Agents for Evolution - The creativity of engineers to come up with answers where there were problems--- without solutions, there would be no Evolution. - Agent I: New Requirements - The demand for higher performance dictated that fetching one instruction at a time was not enough. - Examples like this forces evolution and creative solution thinking. - Agent II: Bottlenecks - We have 3 components of instruction processing (instruction supply, data supply, and carrying out the operations of the instruction). - By far, most of the improvements to the microprocessor have come about due to attempts to eliminate bottlenecks that prevent these components from being fully utilized. - Agent III: Good Fortune - Good Fortune happens when something causes a windfall which can then be used to provide additional features to the microprocessor. # References - [Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution](https://course.ece.cmu.edu/~ece740/f13/lib/exe/fetch.php?media=r0_patt.pdf) ",
    "url": "/markdown_files/posts/content/comparchreads1.html",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html"
  },"9": {
    "doc": "Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems",
    "title": "Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems",
    "content": "# Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems **Thomas Moscibroda** **Onur Mutlu** @microsoft ## Introduction - The transition from single-core to multi-core systems has introduced major performance and security challenges. - Multiple programs running on shared DRAM systems can interfere with each other's memory accesses, leading to performance degradation and security vulnerabilities. - This paper introduces a new security problem that arises due to the core design of multi-core architectures – a denial of service (DoS) attack that was not possible in single-core systems. - An aggressive memory-intensive program can severely impact the performance of other threads with which it is co-scheduled. This is called a Memory Performance Hog (MPH). - This problem worsens with an increasing number of cores, as the impact grows exponentially. - An MPH can be used to perform DoS attacks that fool users into thinking other applications are inherently slow, even without causing easily observable performance issues. - A regular application can unintentionally behave like an MPH and damage the memory-related performance of co-scheduled threads. ## DRAM Architectures - DRAM memory is an expensive resource in modern systems. Creating a separate DRAM system for each core is not feasible. - In a partitioned DRAM system, a processor accessing a memory location needs to issue a request to the DRAM partition that contains the data for that location. ## DRAM Memory Systems ![DRAM BANK ORGANIZATION](/markdown_files/posts/assets/DRAM_Block_diagram.png) - **Row Hit:** Accessing a row already in the row-buffer. It has the lowest latency (around 40-50 ns in commodity DRAM). - **Row Conflict:** Accessing a different row than the one currently in the row-buffer, requiring the row-buffer to be written back before the new row can be accessed. - **Row Closed:** No row in the row-buffer, necessitating a read from the memory array before column access. ## DRAM Controller - The DRAM controller mediates between on-chip caches and off-chip DRAM memory. It receives read/write requests from L2 caches. - The **memory access scheduler** is responsible for selecting memory requests from the memory request buffer to send to the DRAM memory. ## Memory Access Scheduling Algorithm - Current memory access schedulers typically employ the **First-Ready First-Come-First-Serve (FR-FCFS)** algorithm, which prioritizes requests in the following order: - **Row-hit first:** Prioritizes requests that hit the row already in the row-buffer. - **Oldest-within-bank first:** Prioritizes requests that arrived earliest within the same bank. - **Oldest-across-banks first:** Prioritizes the earliest arrival time among requests selected by individual bank schedulers. ## Vulnerabilities of Multi-Core DRAM Memory System to DoS Attacks - Current DRAM memory systems do not distinguish between the requests of different threads. - **Unfairness of Row-Hit First Scheduling:** A thread whose accesses result in row hits gets higher priority compared to a thread whose accesses result in row conflicts. - **Unfairness of Oldest-First Scheduling:** Oldest-first scheduling implicitly favors threads that can generate memory requests at a faster rate than others. ## Examples of DoS in Existing Multi-Cores - When two threads use different access patterns, such as one streaming data and the other accessing memory randomly, the memory controller will prioritize the one with the optimized memory access pattern. ## Fairness in DRAM Memory Systems - Defining fairness in DRAM systems is complex, and coming up with a reasonable definition is challenging. ## Fair Memory Scheduling: A Model - The authors propose a model for fair memory scheduling to mitigate the impact of MPHs. - **Fairness Definition:** A memory scheduler is fair if equal-priority threads experience the same memory-related slowdowns when running together. - **Stall-Time Fair Memory Scheduler (STFM):** STFM prioritizes threads based on their stall times, ensuring that no thread monopolizes memory resources, thus promoting fairness. - **Implementation Considerations:** STFM requires modifications to the memory controller to track stall times for each thread, ensuring equitable memory access for co-scheduled threads. ## Conclusion - The paper highlights the vulnerabilities of multi-core systems to DoS attacks due to unfair memory access scheduling. - By introducing the concept of Memory Performance Hogs and the Stall-Time Fair Memory Scheduler, the authors offer a framework to enhance fairness and improve both the performance and security of multi-core systems. ## References - The paper is written by Professor Onur Mutlu which can be found [here](https://users.ece.cmu.edu/~omutlu/pub/mph_usenix_security07.pdf) ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html"
  }
}
