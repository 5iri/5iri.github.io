{"0": {
    "doc": "Abstract Algebra",
    "title": "What is Abstract Algebra?",
    "content": "Abstract Algebra is the fascinating study of algebraic structures that generalize the arithmetic of numbers. It’s a field that revolutionized mathematics in the 19th century and continues to be fundamental to many branches of mathematics today. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-abstract-algebra",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-abstract-algebra"
  },"1": {
    "doc": "Abstract Algebra",
    "title": "Introduction to Groups",
    "content": "The roots of Abstract Algebra can be traced back to the brilliant minds of the 19th century. The famous mathematician Évariste Galois laid its foundation when he discovered solutions for higher-degree polynomials using groups. Simultaneously, Carl Friedrich Gauss was formalizing the concept of modular arithmetic, which would become a cornerstone in Abstract Algebra. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#introduction-to-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#introduction-to-groups"
  },"2": {
    "doc": "Abstract Algebra",
    "title": "Technical Terms",
    "content": "Before we dive deeper, let’s understand some essential terminology: . | Element: An object in a set. | Set: A collection of objects. | Binary Operation: A rule that combines two elements of a set to produce another element. | Identity element: An element that leaves others unchanged when combined with them. For example, \\(e\\) is the identity element if \\(a * e = e * a = a\\) for all \\(a\\) in the set. | Inverse Element: For each element \\(a\\) in the set, there exists an element \\(a^{-1}\\) such that \\(a * a^{-1} = a^{-1} * a = e\\). | Order of a Group: The number of elements in a group, represented as \\(\\|G\\|\\). | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#technical-terms",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#technical-terms"
  },"3": {
    "doc": "Abstract Algebra",
    "title": "Definition of a Group",
    "content": "At the heart of Abstract Algebra is the concept of a Group. A Group is a set \\(G\\) with a binary operation \\(*\\) that satisfies four fundamental properties: . | Closure: For all \\(a, b \\in G\\), the element \\(a * b\\) is also in \\(G\\). (\\(x,y \\in G \\Rightarrow x * y \\in G\\)) | Associativity: For all \\(a, b, c \\in G\\), \\((a * b) * c = a * (b * c)\\) | Identity Element: There exists an element \\(e \\in G\\) such that for all \\(a \\in G\\), \\(a * e = e * a = a\\) | Inverse Element: For each $a \\in G$, there exists an element \\(a^{-1} \\in G\\) such that \\(a * a^{-1} = a^{-1} * a = e\\) | . While this formal definition might seem abstract, the concept becomes more intuitive through examples: . | The set of integers under addition forms a group. | The set of rotations or flips of a triangle forms a group. | . It’s important to note that a group \\(G\\) may not be commutative, meaning \\(x * y \\neq y * x\\). A group that is commutative is called Abelian, and one that isn’t is called Non-Abelian. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#definition-of-a-group",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#definition-of-a-group"
  },"4": {
    "doc": "Abstract Algebra",
    "title": "What is a Subgroup?",
    "content": "If we have a group \\(G\\) with a binary operation \\(*\\), a subset \\(H\\) of \\(G\\) is a subgroup of \\(G\\) if it is itself a group under the operation \\(*\\). We denote this as \\(H \\leq G\\), meaning \\(H\\) is a subgroup of \\(G\\). If \\(H\\) is a proper subgroup of \\(G\\) (meaning \\(H \\neq G\\)), we write \\(H &lt; G\\). ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-a-subgroup",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-a-subgroup"
  },"5": {
    "doc": "Abstract Algebra",
    "title": "Group Multiplication Tables (Cayley’s Tables)",
    "content": "To visualize how elements of a group interact under the group operation, we use Cayley Tables. Consider the group under multiplication consisting of four elements: \\(* , {-1, 1, i, -i}\\) . \\[\\begin{array}{c|cccc} \\times &amp; 1 &amp; -1 &amp; i &amp; -i \\\\ \\hline 1 &amp; 1 &amp; -1 &amp; i &amp; -i \\\\ -1 &amp; -1 &amp; 1 &amp; -i &amp; i \\\\ i &amp; i &amp; -i &amp; -1 &amp; 1 \\\\ -i &amp; -i &amp; i &amp; 1 &amp; -1 \\end{array}\\] Some interesting features of Cayley Tables: . | The row and column of the identity element are identical. | Each row and column will contain each element of the group exactly once. | This is because each element must have an inverse. | The table is symmetric about the main diagonal for abelian groups. | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#group-multiplication-tables-cayleys-tables",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#group-multiplication-tables-cayleys-tables"
  },"6": {
    "doc": "Abstract Algebra",
    "title": "Cosets and Lagrange’s Theorem",
    "content": "Cosets are subsets of a group that partition the group into equal-sized sets. They’re a crucial concept for understanding the structure of groups. There are two standard subgroups of any group \\(G\\): . | \\(G\\) itself, the entire group. | The Trivial Group, consisting of just the identity element: {\\(e\\)}. | . Lagrange’s Theorem . One of the most elegant results in group theory is Lagrange’s Theorem. It states that: . The order of a subgroup divides the order of the group. Mathematically: If \\(H \\leq G \\Rightarrow \\|G\\| \\ \\text{divides} \\ \\|H\\|\\) . It’s important to note that the converse of Lagrange’s Theorem is not true. Proof of Lagrange’s Theorem . Let’s prove this beautiful theorem step by step: . Assume \\(G\\) is a finite group with \\(\\|G\\| = n\\). Case 1: Let \\(\\{e\\} \\leq G\\) and \\(\\|{e}\\| = 1\\). \\(\\|G\\| = n = 1 \\times n\\), which is true. Case 2: Let \\(\\|G\\| = n\\) and \\(G \\leq G\\). \\(\\|G\\| = n = n \\times 1\\), which is true. Case 3: Let \\(\\|H\\| = k\\) and \\(H &lt; G\\) and \\(H \\neq \\{e\\}\\). Construction: . | Pick \\(g_1 \\ \\epsilon \\ G\\) not in \\(H\\) | \\[g_1H \\ = \\ \\{g_1.h \\ \\text{for} \\ \\text{all}\\ h \\ \\epsilon \\ H \\}\\] | \\(H\\) and \\(g_1\\) do not overlap, i.e., \\(H \\cap g_1H = \\phi\\) | . Let’s prove this by contradiction. Assume there is an element in both \\(H\\) and \\(g_1H\\), meaning \\(g_1 \\cdot h_i = h_j\\) for some elements \\(h_i, h_j\\) in \\(H\\). \\[g_1 \\cdot h_i = h_j\\] \\[(g_1 \\cdot h_i) \\cdot h_i^{-1} = h_j \\cdot h_i^{-1}\\] \\[g_1 \\cdot (h_i \\cdot h_i^{-1}) = h_j \\cdot h_i^{-1}\\] \\[g_1 \\cdot e = h_j \\cdot h_i^{-1}\\] \\[\\Rightarrow g_1 = h_j \\cdot h_i^{-1} \\epsilon \\ H \\Rightarrow \\ g_1 \\epsilon \\ H\\] This contradicts our initial choice of \\(g_1\\) as not being in \\(H\\). We can repeat this process for different cosets \\(g_i\\) that don’t overlap with each other or with \\(H\\). Since all cosets \\(g_iH\\) have the same size as \\(H\\), and they partition \\(G\\), we get: . Number of cosets = \\(k\\) . Index of \\(H\\) in \\(G\\) = \\(\\|G:H\\| = k\\) . So \\(\\|H\\| \\cdot k = \\|G\\| \\Rightarrow \\|H\\| \\text{ divides } \\|G\\|\\) . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#cosets-and-lagranges-theorem",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#cosets-and-lagranges-theorem"
  },"7": {
    "doc": "Abstract Algebra",
    "title": "Normal Subgroups and Quotient Groups",
    "content": "Definition: . A subgroup \\(N\\) of a group \\(G\\) is normal if \\(g \\cdot n \\cdot g^{-1} \\in N\\) for all \\(g \\in G\\) and \\(n \\in N\\). Simple Example: . Consider \\(S_3\\), the group of permutations on 3 elements. The subgroup \\(A₃ = \\{e, (123), (132)\\}\\) consisting of only the even permutations is normal in \\(S₃\\). To verify: If we take any permutation in \\(S₃\\) and conjugate elements of \\(A₃\\) by it, we stay within \\(A₃\\). For instance: . | \\[(12)(123)(12^{-1}) = (12)(123)(12) = (132) \\in A₃\\] | \\[(12)(e)(12^{-1}) = e \\in A₃\\] | . This works because conjugation preserves the “evenness” of permutations. Another Example: . Consider \\(Z\\) to be the set of Integers. Group: Integers \\(Z\\) under addition Subgroups: \\(Z, 2Z, 3Z, 4Z, 5Z, ...\\) . The Group \\(Z\\) consists of elements like \\(5Z, 1+5Z, 2+5Z, 3+5Z, 4+5Z\\), where \\(5Z\\) is the only group, and others are cosets (as they don’t contain the identity element \\(e\\)). Hence, \\(5Z\\) is called the Normal Subgroup. The collection of these cosets forms the Quotient Group, represented by \\(Z/5Z\\). It’s worth noting that \\(Z/5Z \\leq Z\\). Simple Groups: . Every group \\(G\\) has at least 2 subgroups: {\\(e\\)} and \\(G\\) itself. If the only normal subgroups of \\(G\\) are the identity and the group itself, then \\(G\\) is called a simple group. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#normal-subgroups-and-quotient-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#normal-subgroups-and-quotient-groups"
  },"8": {
    "doc": "Abstract Algebra",
    "title": "What are Cyclic Groups?",
    "content": "A group \\(G\\) is cyclic if it is “generated” by a single element. To understand what “generated” means, let’s consider an example: . Let \\(G\\) be a group with operation \\(\\times\\) . | Pick \\(x \\in G\\) | \\(\\langle x \\rangle = \\{ ..., x^{-n}, ..., x^{-2}, x^{-1}, 1, x, x^2, x^3, ..., x^n\\}\\) are all elements generated by \\(x\\). | . If this set equals \\(G\\), then \\(G = \\langle x \\rangle\\), making \\(G\\) a cyclic group. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#what-are-cyclic-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#what-are-cyclic-groups"
  },"9": {
    "doc": "Abstract Algebra",
    "title": "What is Homomorphism?",
    "content": "Homomorphisms allow us to compare different groups, even with different operations. Suppose we have two groups \\((G, *)\\) and \\((H, \\square)\\). Pick two elements \\(x, y \\in G\\) So, \\(x * y \\in G\\) . A function \\(f: G \\to H\\) is a homomorphism if: . | It maps \\(x\\) to \\(f(x)\\) in \\(H\\) | It maps \\(y\\) to \\(f(y)\\) in \\(H\\) | And crucially, \\(f(x * y) = f(x) \\square f(y)\\) | . This gives us a way to compare two entirely different groups with different operations! . Definition: A homomorphism is a function that preserves the structure of the groups. Note: Homomorphisms don’t need to be one-to-one functions. An Isomorphism is a special type of homomorphism that is both one-to-one and onto (a bijection). This means the function that maps \\(G\\) to \\(H\\) is unique. What is a kernel of a Group Homomorphism? . Given a homomorphism \\(f: G \\to H\\) that may not be one-to-one, multiple elements from \\(G\\) might map to the same element in \\(H\\). The kernel of \\(f\\) is defined as: ker(\\(f\\)) = \\(\\{x \\in G \\mid f(x) = 1_H\\}\\) . The kernel tells us the degree to which the function fails the one-to-one condition. It’s important to note that ker(\\(f\\)) is never empty, as even when \\(f\\) is one-to-one, ker(\\(f\\)) = \\(\\{1_G\\}\\). This is also the condition for \\(f\\) to be one-to-one. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-homomorphism",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#what-is-homomorphism"
  },"10": {
    "doc": "Abstract Algebra",
    "title": "Symmetric Groups",
    "content": "\\(S_n\\) is the group of permutations on a set with \\(n\\) elements. It’s a finite group with \\(n!\\) elements. All symmetric groups with \\(n &gt; 2\\) are non-abelian. An important result in group theory is Cayley’s Theorem, which states that every finite group is isomorphic to a subgroup of a symmetric group. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#symmetric-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#symmetric-groups"
  },"11": {
    "doc": "Abstract Algebra",
    "title": "Cycle notation of permutations",
    "content": "This is a method to represent permutations in a more concise form. Simple Example: . Consider the symmetric group \\(S_4\\). A permutation in this group can be written as: . \\[\\begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 1 &amp; 4 &amp; 3 \\end{pmatrix}\\] This means: . | \\(1\\) maps to \\(2\\) | \\(2\\) maps to \\(1\\) | \\(3\\) maps to \\(4\\) | \\(4\\) maps to \\(3\\) | . Similarly, another permutation might be: . \\[\\begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 3 &amp; 4 &amp; 1 &amp; 2 \\end{pmatrix}\\] These can be viewed as functions. If we call the first permutation \\(f\\) and the second \\(g\\), then: . | \\(f(1) = 2\\), \\(f(2) = 1\\), \\(f(3) = 4\\), \\(f(4) = 3\\) | \\(g(1) = 3\\), \\(g(2) = 4\\), \\(g(3) = 1\\), \\(g(4) = 2\\) | . The group operation is function composition, \\(f \\circ g\\), which gives: . \\[f \\circ g = \\begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{pmatrix}\\] But writing permutations in this matrix form can be cumbersome, especially for larger groups. That’s where cycle notation comes in. In cycle notation, we write: . | \\[f = (1,2)(3,4)\\] | \\[g = (1,3)(2,4)\\] | . This makes it much easier to read and write permutations. The notation $(a, b, c, …)$ means \\(a\\) maps to \\(b\\), which maps to \\(c\\), and so on, with the last element mapping back to \\(a\\). Sidenote: The order of the cycles in a permutation doesn’t matter, and the order of elements within a cycle (except for the starting point) doesn’t matter either. So $(1,2)(4,3) = (3,4)(1,2)$. Two-element cycles, like $(1,2)$, are called transpositions. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#cycle-notation-of-permutations",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#cycle-notation-of-permutations"
  },"12": {
    "doc": "Abstract Algebra",
    "title": "Dihedral Group",
    "content": "The word Dihedral means “two faces.” These groups describe the symmetries of regular polygons. There are three types of operations a regular \\(n\\)-sided polygon can undergo: . | Rotation (denoted as \\(r\\)) | Flip (denoted as \\(f\\)) | No change (the identity element \\(e\\)) | . For an equilateral triangle: . | No change in position is the identity element \\(e\\). | Rotating by \\(120^\\circ\\) is denoted as \\(r\\). | Rotating by \\(240^\\circ\\) can be denoted as \\(r^2\\) (rotating twice). | Rotating by \\(360^\\circ\\) brings us back to the starting position, so \\(r^3 = e\\). | Flipping the triangle vertically is denoted as \\(f\\). | Flipping twice returns to the original position, so \\(f^2 = e\\). | . The total distinct transformations for the triangle are: \\(e, r, r^2, f, r \\cdot f, r^2 \\cdot f\\) . Also, note that the order of \\(r\\) is 3 (\\(\\|r\\| = 3\\)) and the order of \\(f\\) is 2 (\\(\\|f\\| = 2\\)). ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#dihedral-group",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#dihedral-group"
  },"13": {
    "doc": "Abstract Algebra",
    "title": "Matrix Groups",
    "content": "Groups formed using matrices are called Matrix Groups. The two common operations with matrices are addition and multiplication. Under addition, the identity element for an \\(n \\times m\\) matrix is a matrix with all elements as 0 (denoted as \\(O\\)). For matrices under multiplication, the group is typically limited to \\(n \\times n\\) matrices. The identity element is the identity matrix \\(I\\). For a matrix to have an inverse (and hence be part of a group under multiplication), its determinant must be non-zero, i.e., det(\\(M\\)) \\(\\neq\\) 0. Some important matrix groups include: . | General Linear Group (\\(GL_n(R)\\)): The group of \\(n \\times n\\) invertible matrices. | Special Linear Group (\\(SL_n(R)\\)): The group of \\(n \\times n\\) invertible matrices with determinant 1. | . The \\(R\\) indicates that the matrix elements are real numbers. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#matrix-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#matrix-groups"
  },"14": {
    "doc": "Abstract Algebra",
    "title": "Direct products of Groups",
    "content": "Just as integers can be decomposed into products of primes, groups can be broken down into simpler groups. One way to build complex groups from simpler ones is through direct products. Given two groups \\(G_1\\) and \\(G_2\\), their direct product is defined as: . \\[G_1 \\times G_2 = \\{(x,y) \\mid x \\in G_1, y \\in G_2\\}\\] The direct product of multiple groups is also possible: . \\[G_1 \\times G_2 \\times G_3 \\times \\ldots \\times G_n = \\{(x_1, x_2, x_3, \\ldots, x_n) \\mid x_1 \\in G_1, x_2 \\in G_2, \\ldots, x_n \\in G_n\\}\\] The order of the direct product is the product of the orders of its constituent groups: . \\[\\|G_1 \\times G_2 \\times \\ldots \\times G_n\\| = \\|G_1\\| \\times \\|G_2\\| \\times \\ldots \\times \\|G_n\\|\\] The identity element of the direct product is the tuple of identity elements from each group: . \\((e_1, e_2, e_3, \\ldots, e_n)\\), where \\(e_i\\) is the identity element of \\(G_i\\). If any of the groups in the direct product is non-abelian, then the direct product itself is also non-abelian. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#direct-products-of-groups",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#direct-products-of-groups"
  },"15": {
    "doc": "Abstract Algebra",
    "title": "Simple Groups (In Detail)",
    "content": "A subgroup \\(N\\) of a group \\(G\\) is normal if \\(g \\cdot N \\cdot g^{-1} = N\\) for all \\(g \\in G\\). A group \\(G\\) is simple if its only normal subgroups are {1} and \\(G\\) itself. Let’s consider a finite group \\(G\\) with a normal subgroup \\(N_1 \\triangleleft G\\). We say \\(N_1\\) is maximal and proper if: . Maximal: There’s no group \\(H\\) between \\(G\\) and \\(N_1\\), i.e., no group \\(H\\) with \\(N_1 \\subset H \\subset G\\). Proper: \\(N_1 \\neq G\\). We can continue this process, finding a normal subgroup of \\(N_1\\), and so on, forming a normal series: . \\[1 \\triangleleft N_i \\triangleleft N_{i-1} \\triangleleft \\ldots \\triangleleft N_2 \\triangleleft N_1 \\triangleleft G\\] When this series is made as long as possible, it’s called a composition series. Jordan-Hölder Theorem . The Jordan-Hölder Theorem states that if there are multiple composition series for a finite group \\(G\\), they are equivalent in the sense that they have the same length and identical factor groups. Classification of Finite Simple Groups . In a composition series, each quotient group is a simple group. This means that understanding all finite groups can be broken down into two tasks: . | Find all simple groups. | Find all extensions of simple groups. | . This leads to the extension problem: Given a finite group \\(N\\) and a simple group \\(S\\), find all groups \\(G\\) where \\(N \\triangleleft G\\) and \\(G/N \\cong S\\). Researchers have made significant progress in classifying all finite simple groups. There are four categories: . | Cyclic groups of prime order: \\(Z/pZ\\) where \\(p\\) is prime. These are simple because their only divisors are 1 and \\(p\\), which means their only subgroups are {0} and the group itself. | Alternating groups \\(A_n\\) for \\(n \\geq 5\\). These relate to the non-existence of general formulas for solving polynomial equations of degree 5 or higher. | Groups of Lie Type: These are related to manifolds, spaces that locally resemble Euclidean space. One example is the group of complex numbers with absolute value 1, which forms a circle. Another is the group of real, invertible \\(n \\times n\\) matrices. | 26 Sporadic Groups: These are exceptional groups that don’t fit into the other categories. Among them, the Monster Group is the largest, with approximately \\(8 \\times 10^{53}\\) elements. The Monster contains 20 of the 26 sporadic groups, known as the “happy family,” while the remaining 6 are called the “pariahs.” . | . The classification of finite simple groups is one of the monumental achievements of modern mathematics, spanning approximately 10,000 pages of research. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#simple-groups-in-detail",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#simple-groups-in-detail"
  },"16": {
    "doc": "Abstract Algebra",
    "title": "The Definition of a Ring",
    "content": "A Ring is a set of elements with operations similar to addition and multiplication, though the inverses of these operations may not always be defined. For example, a \\(2 \\times 2\\) matrix with real elements forms a ring. A ring is always “closed,” meaning the result of any operation on ring elements is also in the ring. Technically, a ring is a set \\(R\\) with two operations, addition (\\(+\\)) and multiplication (\\(\\times\\)), satisfying: . | Closure: For \\(x, y \\in R\\), both \\(x + y \\in R\\) and \\(x \\times y \\in R\\). | The set forms a commutative group under addition. | Multiplication is associative. | Multiplication is distributive over addition. | . If a ring is commutative under multiplication, it’s called a commutative ring. If a ring contains a multiplicative identity (usually denoted as 1), it’s called a ring with identity. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#the-definition-of-a-ring",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#the-definition-of-a-ring"
  },"17": {
    "doc": "Abstract Algebra",
    "title": "Examples of Rings",
    "content": "1. The Set of Integers (\\(Z\\)) . The set of all positive and negative whole numbers, including zero, forms a ring under the operations of addition and multiplication. Properties: . | Closure: Adding or multiplying two integers gives another integer. | Associativity: Both addition and multiplication are associative. | Additive Identity: 0 serves as the additive identity. | Multiplicative Identity: 1 serves as the multiplicative identity. | Additive Inverses: For every integer \\(a\\), there’s an integer \\(-a\\) such that \\(a + (-a) = 0\\). | Distributivity: Multiplication distributes over addition. | . 2. The Set of Polynomials with Real Coefficients (\\(R[x]\\)) . This is the set of all polynomials where the coefficients are real numbers. Properties: . | Closure: The sum and product of two polynomials in \\(R[x]\\) are also in \\(R[x]\\). | Associativity: Both addition and multiplication are associative. | Additive Identity: The zero polynomial is the additive identity. | Multiplicative Identity: The polynomial 1 is the multiplicative identity. | Additive Inverses: For every polynomial \\(p(x)\\), there’s a polynomial \\(-p(x)\\) such that \\(p(x) + (-p(x)) = 0\\). | Distributivity: Multiplication distributes over addition. | . 3. The Set of \\(n \\times n\\) Matrices with Real Entries (\\(M_n(R)\\)) . This is the set of all \\(n \\times n\\) matrices where each entry is a real number. Properties: . | Closure: The sum and product of two \\(n \\times n\\) matrices are also \\(n \\times n\\) matrices. | Associativity: Matrix addition and multiplication are associative. | Additive Identity: The zero matrix is the additive identity. | Multiplicative Identity: The identity matrix is the multiplicative identity. | Additive Inverses: For every matrix \\(A\\), there’s a matrix \\(-A\\) such that \\(A + (-A) = 0\\). | Distributivity: Matrix multiplication distributes over addition. | . 4. The Set of Continuous Functions from \\(R\\) to \\(R\\) (\\(C(R, R)\\)) . This is the set of all continuous functions mapping real numbers to real numbers. Properties: . | Closure: The sum and product (pointwise) of two continuous functions are continuous. | Associativity: Function addition and multiplication are associative. | Additive Identity: The zero function (\\(f(x) = 0\\) for all \\(x\\)) is the additive identity. | Multiplicative Identity: The constant function \\(f(x) = 1\\) is the multiplicative identity. | Additive Inverses: For every function \\(f\\), there’s a function \\(-f\\) such that \\(f + (-f) = 0\\). | Distributivity: Function multiplication distributes over addition. | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#examples-of-rings",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#examples-of-rings"
  },"18": {
    "doc": "Abstract Algebra",
    "title": "Units in a Ring",
    "content": "Definition of Units . A unit in a ring is an element that has a multiplicative inverse. If \\(a\\) is an element in a ring \\(R\\) and there exists a \\(b \\in R\\) such that \\(a \\times b = 1\\), then \\(a\\) is a unit, and \\(b\\) is its inverse. Group of Units . The set of all units in a ring forms a group under multiplication, known as the group of units. Examples . | In the ring of integers (\\(Z\\)), the only units are 1 and -1, as these are the only integers with multiplicative inverses within \\(Z\\). | In the ring of real numbers (\\(R\\)), every non-zero real number is a unit, since each has a multiplicative inverse. | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#units-in-a-ring",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#units-in-a-ring"
  },"19": {
    "doc": "Abstract Algebra",
    "title": "Integral Domains",
    "content": "An Integral Domain is a commutative ring \\(R\\) with a multiplicative identity and no zero divisors. This means that if \\(a \\times b = 0\\), then either \\(a = 0\\) or \\(b = 0\\). A key property of integral domains is that they satisfy the cancellation law: If \\(a \\times b = a \\times c\\) and \\(a \\neq 0\\), then \\(b = c\\). ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#integral-domains",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#integral-domains"
  },"20": {
    "doc": "Abstract Algebra",
    "title": "Ideals in Rings",
    "content": "An Ideal is a subset of a ring that is closed under addition and multiplication by elements of the ring. It’s analogous to how subgroups relate to groups. Formally, if \\(I\\) is an ideal of a ring \\(R\\), then: . | For all \\(a, b \\in I\\), \\(a + b \\in I\\) (closed under addition) | For all \\(a \\in I\\) and \\(r \\in R\\), \\(a \\times r \\in I\\) and \\(r \\times a \\in I\\) (closed under multiplication by ring elements) | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#ideals-in-rings",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#ideals-in-rings"
  },"21": {
    "doc": "Abstract Algebra",
    "title": "Key Concepts",
    "content": "| Concept | Description | . | Normal Subgroup | Subgroup N of G, forms factor group G/N. | . | Ring | Abelian group under addition, associative multiplication. | . | Ideal | Additive subgroup I of R, r·i, i·r ∈ I. | . | Factor Ring | Cosets of R modulo I form ring R/I. | . | Left Ideals and Right Ideals also exist and are important in non-commutative rings. | The intersection of two ideals is also an ideal. | The sum of two ideals is also an ideal. | The product of two ideals is also an ideal. | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#key-concepts",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#key-concepts"
  },"22": {
    "doc": "Abstract Algebra",
    "title": "What are Fields?",
    "content": "A Field is a ring where all non-zero elements have multiplicative inverses. This means it can perform all the operations familiar from arithmetic. Formally, a field is a set \\(F\\) with two operations, addition and multiplication, such that: . | The set forms a commutative group under addition. | The set of non-zero elements forms a commutative group under multiplication. | Multiplication is distributive over addition. | . Prime Fields . The smallest fields are called prime fields. These are fields of integers modulo a prime number, like \\(Z/2Z\\), \\(Z/3Z\\), \\(Z/5Z\\), and so on. The characteristic of a field is the smallest positive integer \\(n\\) such that \\(n \\times 1 = 0\\). If no such \\(n\\) exists, the field has characteristic 0. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#what-are-fields",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#what-are-fields"
  },"23": {
    "doc": "Abstract Algebra",
    "title": "Vector Spaces",
    "content": "A Vector Space is a set of elements (vectors) along with operations of vector addition and scalar multiplication. The scalars come from a field. A vector space over a field \\(F\\) must satisfy several axioms, including: . | The set of vectors forms a commutative group under addition. | Scalar multiplication is distributive over vector addition. | Scalar multiplication is compatible with field multiplication. | The multiplicative identity of the field acts as a scalar that leaves vectors unchanged. | . ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#vector-spaces",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#vector-spaces"
  },"24": {
    "doc": "Abstract Algebra",
    "title": "Modules",
    "content": "A Module generalizes the concept of a vector space by replacing the field of scalars with a ring. This allows for a broader study of algebraic structures. Formally, a module over a ring \\(R\\) consists of a set of elements (like vectors) together with operations of addition and scalar multiplication by elements of \\(R\\), satisfying axioms similar to those of a vector space. The key difference between a module and a vector space is that modules allow for scalar multiplication from a ring, while vector spaces require scalars from a field. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#modules",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#modules"
  },"25": {
    "doc": "Abstract Algebra",
    "title": "References",
    "content": ". | A YouTube playlist by Socratica | A Book of Abstract Algebra - Charles C. Pinter | Abstract Algebra - Theory and Applications - Judson, Stephen | . I hope this guide has provided a comprehensive introduction to the beautiful world of Abstract Algebra. From the foundational concept of groups to the more advanced topics of rings, fields, and modules, Abstract Algebra offers a fascinating glimpse into the deeper structures that underlie mathematics. ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html#references",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html#references"
  },"26": {
    "doc": "Abstract Algebra",
    "title": "Abstract Algebra",
    "content": " ",
    "url": "/markdown_files/posts/content/Abstract_Algebra.html",
    
    "relUrl": "/markdown_files/posts/content/Abstract_Algebra.html"
  },"27": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Understanding the Basics",
    "content": "The ApexCore project is an immersive journey into the world of computer architecture and organization (COA). Although the subject is vast and intricate, we, Shri Vishakh Devanand, Aditya Mahajan, and Shaurya Rane, began our exploration by understanding the core components of a CPU. We started with an edX course that, despite being taught in TLVerilog (a newer and less common programming language), provided us with the foundational knowledge necessary to progress. ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#understanding-the-basics"
  },"28": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "A simple breakdown of the CPU",
    "content": "To understand the logic of how it works, let’s go through the diagram with the flow. | Imagine a 32-bit instruction is going through to the CPU. | Instructions are stored in Instruction Memory and they are accessed via addresses stored in Program Counter. The instruction received by the decoder to decode the instruction into what generally occurs into 3 or 4 different things. The instruction type is defined by the operation code. | The source values are given as the address of its place in the register, rather than the value itself, or for some instructions there comes something called immediate values, where you don’t want to store the value but just use the value for another calculation and lose it. | Then the destination register, where the calculated value is going to be stored until it is stored in memory or discarded. | Then, it goes into the control unit, where it decides what type of calculation is required for this particular instruction and sends it to the ALU to calculate it. | . | Once the final result is received by the control unit, it is then stored in the destination register as instructed. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#a-simple-breakdown-of-the-cpu"
  },"29": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "What have we done so far",
    "content": "We have completed testing RV32-IM extensions under simulation, which introduces new instructions for multiplication and division. Here’s a detailed breakdown of our progress: . | Code Optimization: We revisited the M extension to refine the code, focusing on enhancing efficiency and ensuring proper functionality. | Simulation: Using Vivado, we simulated the RV32-IM extensions. The simulation verified that the new instructions for multiplication and division were correctly implemented and functioning as expected. | Debugging and Validation: During simulation, we meticulously debugged the code to identify and fix any issues. This process was crucial for validating the correctness of our implementation. | Documentation: Alongside coding and simulation, we documented our process and findings. This documentation will serve as a valuable reference for future development and troubleshooting. | Preparation for Hardware Testing: We have prepared the code for the upcoming hardware testing phase. This includes ensuring compatibility with our target FPGA platform and setting up the necessary testing environment. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#what-have-we-done-so-far"
  },"30": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "Plans for the coming days",
    "content": ". | We are about to test the current code on hardware and ensure it works with high confidence. | Keep adding more extensions like Atomic and Floating-point extensions to further increase the general usage of the core. | Further End Goal of the project to create a soft-core that can be used for other projects, who don’t need to build a cpu from scratch in order to do so. | . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html#plans-for-the-coming-days"
  },"31": {
    "doc": "ApexCore A RISC-V based CPU",
    "title": "ApexCore A RISC-V based CPU",
    "content": "–Shri Vishakh Devanand –Aditya Mahajan –Shaurya Rane . ",
    "url": "/markdown_files/posts/content/ApexCore_blog.html",
    
    "relUrl": "/markdown_files/posts/content/ApexCore_blog.html"
  },"32": {
    "doc": "Multi-Agent Path Finding Algorithms",
    "title": "Multi-Agent Path Finding",
    "content": ". ",
    "url": "/markdown_files/posts/content/MAPF.html#multi-agent-path-finding",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html#multi-agent-path-finding"
  },"33": {
    "doc": "Multi-Agent Path Finding Algorithms",
    "title": "Introduction",
    "content": ". | MAPF is a planning problem in which the task is to plan paths for multiple agents, where the key constraint is that the agents will be able to follow these paths while not colliding with each other. | This problem has a range of research groups and academic communities which have studied them but they all have very different terminoligies used, and someof them have also different objectives in their paper. | The paper aims to address this issue by introducing unified terminology to describe MAPF problems. | The second part of the paper introduces a new grid MAPF benchmark. | . ",
    "url": "/markdown_files/posts/content/MAPF.html#introduction",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html#introduction"
  },"34": {
    "doc": "Multi-Agent Path Finding Algorithms",
    "title": "Classical MAPF",
    "content": "Description of the classical MAPF . | The input to a classical MAPF problem with k agents is a tuple &lt;G,s,t&gt;, where G = (V, E) [Have some doubts here(1. What is V and E here? V can be assumed to be Vector but I have no idea what E is.) ] is an undirected graph, | . s : [1,…..,k] -&gt; V maps an agent to a source vertex. t : [1,……,k] -&gt; V maps an agent to a target vertex. | Time is assumed to be discretized, and in every step each agent is situated in one of the graph vertices and can perform a single action. | An Action in classical MAPF is a function a : V -&gt; V such that a(v) = v’ means if an agent is at vertex v and performs an action, then it will be in vertex v’ in the next time step. | So Each agent has two types of actions : wait and move. | wait means that the agent stays exactly where it is. | move means the agents moves to another adjacent block (or vertex as written in the paper). | For a sequence of actions represented as π = \\((a_{1}, ..... a_{n})\\), and an agent i, we denote by \\(\\pi_{i} \\left[ x \\right]\\) = \\(a_{x}(a_{x-1}(...a_{1}(s(i)))\\). | \\(\\pi\\) is a single-agent plan for agent \\(i\\) iff executing this sequence of actions in \\(s(i)\\) results in being at \\(t(i)\\), \\(\\pi_{i}[|\\pi |] = t(i)\\) . | A solution is a set of $k$ single-agent plans, one for each agent. This means we could ig say it forms a $k * n$ matrix?? . | . Types of Conflicts in Classical MAPF . | There is a possibility of \\(vertex \\ conflict\\) between \\(\\pi_{i}\\) and \\(\\pi_j\\) occuring when both agents plans to occupy a particular positions. | There is a possibility of \\(edge\\ conflict\\) between \\(\\pi_{i}\\) and \\(\\pi_{j}\\) occuring iff where both the agents plan to traverse to the same edge at the same time and step in the same direction. | Formally, there is an edge conflict between \\(\\pi_{i}\\) and \\(\\pi_{j}\\) iff there exists a time step $x$ such that \\(\\pi_{i} [x] = \\pi_{j} [x]\\) and \\(\\pi_{i}[x+1] = \\pi_{j} [ x+1]\\). /// Don’t know when is this possible, in the sense we already have vertex conflict which basically says that \\(\\pi_{i} = \\pi_{j}\\) right? | . | \\(Following\\ conflict\\) is between \\(\\pi_{i}\\) and \\(\\pi_{j}\\) occurs iff one agent is planned to occupy a vertex that was occupied by another agent in the previous time step. Now, that’s gonna be ignored since occupied positions are something that we are neglecting in here. // Why is this a big issue? . | \\(\\pi_{i} [x+1] = \\pi_{j} [x]\\) mathematically written. | . | A conflict where multiple agents can enter into a position forming a loop like . | . | x | x | x | x | x | x | x | x | x | x | . | x | 1 | 2 | x | x | x | x | x | x | x | . | x | 3 | 4 | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . | x | x | x | x | x | x | x | x | x | x | . – This is called as \\(Cycle\\ conflict\\). | \\[π_{i}(x + 1) = π_{i+1}(x)\\] | \\(Swapping\\ conflict\\) is like the cycle one, where it would just switch the vertices. | Cosidering the formal definitions of these conflicts, it is clear that there are dominance in relation with them, . 1. Forbidding vertex conflicts also implies edge conflicts are forbidden. 2. Forbidding following conflicts also implies cycle conflicts are also forbidden. 3. Forbidding cycle conflicts implies that swapping conflicts are also forbidden. | This is also true vice versa. | To properly define a classical MAPF problem, one needs to specify which types of conflicts are allowed in a solution. | The least constrained restriction is to only forbid edge conflicts. (In our task, ig Swapping and edge are not allowed so only 1 and 3 are forbidden per say). | . Agent Behaviour at Target in Classical MAPF . | There are two ccommon assumptions that are done when defining a classical MAPF problem, . | Stay at target : Under this assumption, an agent waits in its target untill all agents have reached their targets. This will then cause a vertex conflict with any plan that passes through its target after it has reached it. | Disappear at target: Under this assumption, when an agent reaches its target it immediately disappears. This means the plan of that agent will not have any conflict after the time step in which the corresponding agent has reached its target. | . | . Objective Functions in Classical MAPF . | To capture the best solution, we consider something called as objective functions that is used to evaluate MAPF solutions. The two most common functions used for evaluating a solution in classical MAPF are \\(makespan\\) and \\(sum\\ of\\ costs\\). | \\(Makespan\\) : The number of time steps required for all agents to reach their target. | \\(Sum\\ of \\ costs\\): The sum of time steps required by each agent to reach its target. It is also called as flowtime. | . | For example, assume that agent i reaches its target at time step t, leaves its target at time step t ‘ , arrives back at its target at time step t’’, and then stays at its target until all agents reach their target. Then, this single-agent plan will contribute t’’ to the sum of costs of the corresponding solution. | . Beyond Classical MAPF . | As of now, the assumptions taken are: . | time is discretized into time steps, | every action takes exactly one time step, | ",
    "url": "/markdown_files/posts/content/MAPF.html#classical-mapf",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html#classical-mapf"
  },"35": {
    "doc": "Multi-Agent Path Finding Algorithms",
    "title": "in every time step, each agent occupies exactly a single vertex.",
    "content": "MAPF on Weighted Graphs . | . | The notation \\(G\\) now is a weighted graph where the weight of each edge represents the duration it will take an agent to traverse this edge. | Types of graphs that have been included in this paper: . | MAPF in \\(2^k\\)-neighbor grids – These maps are restricted form of weighted graphs in which every vertex represents a cell in a two dimensional grid. DID NOT UNDERSTAND THIS | . | . | MAPF in Euclidean space – These maps are a generalization of MAPF in which every node in \\(G\\) represents a Euclidean point (\\(x\\), \\(y\\)), and the edges represent allowed move actions. Such settings arise when the underlying graph is a roadmap generated for a continuous Euclidean environment. ABSOLUTELY DID NOT UNDERSTAND THIS EITHER. | . Feasibility Rules . | Depending on what you want to lose and what you want in priority, there are sufficient rules for the classical MAPF solution. | Robustness rules – These rules are designed to ensure that a MAPF solution considers inadvertant delays in execution. A \\(k-robust\\) MAPF plan builds in a sufficient buffer for agents to be delayed up to \\(k\\) time steps without resulting in a conflict. This is only when the probability of future delays are known (ig this is where we can use RL if I am not wrong, where we can use ML to predict the probability of a task in a delay. | Formation rules – this is restriction added whenever there is some action that needs to be decided by the bot (or “\\(agent\\)” as given in the paper). | . From Pathfinding to Motion planning . | We are now considering bots now not in vertex but with something as a limited speed and a variety of sizes (\\(configuration\\) is a better word). | MAPF with large agents – This basically now introduces volume to the vertex, and now a single \\(agent\\) can take up multiple vertices. Hence, it may prevent some other bot to move or even take use of those vertices. It may also not allow other \\(agents\\) along the edge whenever one \\(agent\\) is along the edge. | . | There are several approaches to solving these kinds of issues, including a CBS-based approach and a prioritized planning approach. A special case of agents with volume is the convoy setting, in which agents occupy a string of vertices and their connecting edges. | . | MAPF with kinematic constraints – This basically is now introducing another parameter which needs to also think about how to move over to another location without falling or whatever it is not to do to reach the final destination. | A by-product of such contraints is that the underlying graph becomes directed, as there may be edges that can only be passable in one-direction due to kinematic constraints of the agent. | There is some reduction-based approach that assumes rotation actions as a half way to kinematic constraints. | . Tasks and Agents . | In classical MAPF, one agent — one task – to get it to its target. Several extensions have been made in the MAPF literature in which agents may be assigned more than one target. | Anonymous MAPF — The objective is to move the agents to a set of target vertices, but it does not matter which agent reaches which target. (Like, in a place where all the products are the same, it doesn’t matter which person goes and takes the box to be delivered, all we need to know is that that thing is delivered on time. | Colored MAPF — The objective here is to generalize and group the agents into teams, and each team has their own sets of targets. (THIS IS THE TASK WE ARE GOING TO WORK ON). | Another way to view this variant, is as a MAPF problem in which every agent can be assigned to targets only from the set of targets designated for its team. | This can be generalised even further, assigning a target and an agent to multiple teams. | . | Online MAPF — In \\(online\\) MAPF, a sequence of MAPF problems are solved on the same graph. This setting has also been called “Lifelong MAPF”. Online MAPF problems can be classified as follow. | warehouse model – This is the setting where a fixed set of agents, solve a MAPF problem, but after an agent finds a target, it may be tasked to go to different target. This setting is inspired by MAPF for autonomous warehouses. (This is also another way to represent the task since, we do have fixed number of agents for each team, and they are always given some task “$target$”, which once is done, it is made to go to another location to do another task, which is also what is being done, so basically we are doing a mix of colored and Online MAPF). | Interesection model – This is the setting where new agents may appear and each agent has one task – to reach its target. This setting is inspired by autonomous vehicles entering and exiting intersections. | . | . ",
    "url": "/markdown_files/posts/content/MAPF.html#in-every-time-step-each-agent-occupies-exactly-a-single-vertex",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html#in-every-time-step-each-agent-occupies-exactly-a-single-vertex"
  },"36": {
    "doc": "Multi-Agent Path Finding Algorithms",
    "title": "Multi-Agent Path Finding Algorithms",
    "content": " ",
    "url": "/markdown_files/posts/content/MAPF.html",
    
    "relUrl": "/markdown_files/posts/content/MAPF.html"
  },"37": {
    "doc": "Home",
    "title": "Yo!",
    "content": "I’m Shri Vishakh Devanand, a sophomore engineering student from India absolutely obsessed with mathematics, physics and electronics. I thrive at the intersection of hardware and software, applying complex mathematical concepts to solve real engineering challenges. ",
    "url": "/#yo",
    
    "relUrl": "/#yo"
  },"38": {
    "doc": "Home",
    "title": "What I Do",
    "content": ". | Built a fully functional RISC-V CPU on FPGA from scratch | Dive deep into complex analysis, abstract algebra, and discrete mathematics | Developing custom zig based compiler for ESP32. | Create bio-inspired robots that mimic animal behavior. | . ",
    "url": "/#what-i-do",
    
    "relUrl": "/#what-i-do"
  },"39": {
    "doc": "Home",
    "title": "My Vision",
    "content": "Mathematics is my universal language for innovation - whether optimizing algorithms, designing circuits, or modeling control systems. I’m constantly exploring the beautiful intersection where theoretical math meets practical engineering. ",
    "url": "/#my-vision",
    
    "relUrl": "/#my-vision"
  },"40": {
    "doc": "Home",
    "title": "Fun Facts",
    "content": ". | Maintain a personal notebook of elegant mathematical proofs (collecting all of them into a single one is remaining 😅) | Can lose myself for hours in puzzles | Love to challenge myself | . Do check out my Github and LinkedIn profiles to know more about my projects and interests. Feel free to reach out to me for collaborations or discussions on any of the topics you are passionate about. I’m always up for a good chat! . Also check out my blogs on the navigation bar! 😄 . ",
    "url": "/#fun-facts",
    
    "relUrl": "/#fun-facts"
  },"41": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"42": {
    "doc": "Agency and its meaning",
    "title": "What Does Agency Really Mean?",
    "content": "When you hear the word “agency,” you might think of some dictionary definition about taking action to make something happen. But to me, agency is so much more than that—it’s about choosing to act with purpose, chasing the best possible outcome for yourself and those around you, even when the path isn’t clear. For me, agency isn’t just about doing stuff. It’s about why you’re doing it. It’s that moment when you decide to step up, make a choice, and shape what happens next—especially when it’s tough or uncertain. It’s about having the freedom and the guts to say, “I’ve got this,” and mean it. But here’s the thing: agency doesn’t exist in a vacuum. If you’re in a place where you feel unsafe, ignored, or undervalued, it’s hard to feel like you can make a difference. That’s why I think agency is also about creating space—space where people feel heard, supported, and free to take action. One spot where I’ve seen this come to life is in my college club, the Society of Robotics and Automation at VJTI. It’s a place where we build cool things, sure, but more than that, it’s where we lift each other up to try, fail, and try again. So when I talk about agency, I’m not just talking about making moves. I’m talking about setting the stage for action—giving yourself and others a voice, owning your choices, and believing that what you do matters. Even the small stuff counts. ",
    "url": "/markdown_files/posts/content/agency.html#what-does-agency-really-mean",
    
    "relUrl": "/markdown_files/posts/content/agency.html#what-does-agency-really-mean"
  },"43": {
    "doc": "Agency and its meaning",
    "title": "Finding My Own Agency",
    "content": "When I first started thinking about what agency means to me, I had to ask myself: What am I really about? Like, what do I want to stand for? Not just “I want to build robots” or “I want to get a degree,” but something deeper. ",
    "url": "/markdown_files/posts/content/agency.html#finding-my-own-agency",
    
    "relUrl": "/markdown_files/posts/content/agency.html#finding-my-own-agency"
  },"44": {
    "doc": "Agency and its meaning",
    "title": "What Kind of Person Do You Want to Be?",
    "content": "We’ve all heard people say, “I want to be an astronaut!” or “I want to be a doctor!” And those are awesome dreams. But I think the real question is: What kind of person do you want to be, day in and day out? Maybe it’s “I want to be curious,” or “I want to make people feel seen,” or “I want to keep trying new things, no matter what.” . Those kinds of goals—they stick with you. They’re not just about hitting some finish line; they’re about how you show up every single day. When you start thinking like that, you figure out where your agency lives. It’s not just about what you accomplish—it’s about who you are in the moments that matter to you. ",
    "url": "/markdown_files/posts/content/agency.html#what-kind-of-person-do-you-want-to-be",
    
    "relUrl": "/markdown_files/posts/content/agency.html#what-kind-of-person-do-you-want-to-be"
  },"45": {
    "doc": "Agency and its meaning",
    "title": "Agency Is a Compass",
    "content": "So, what’s agency, really? It’s that fire inside you, that stubborn push to make things better—for yourself, for others, for the world. It’s knowing you’ve got the power to shape your path and helping the people around you see they’ve got that power too. Whether you’re tinkering with circuits, leading a group, dreaming up new ideas, or just figuring out who you want to be, your agency is like a compass. It keeps you pointed toward what’s possible. And the more we nurture that spark—in ourselves and in each other—the bigger, brighter, and more incredible the results can be. ",
    "url": "/markdown_files/posts/content/agency.html#agency-is-a-compass",
    
    "relUrl": "/markdown_files/posts/content/agency.html#agency-is-a-compass"
  },"46": {
    "doc": "Agency and its meaning",
    "title": "Agency and its meaning",
    "content": " ",
    "url": "/markdown_files/posts/content/agency.html",
    
    "relUrl": "/markdown_files/posts/content/agency.html"
  },"47": {
    "doc": "Single RISC-V Core",
    "title": "Single RISC-V Core",
    "content": "Redirecting to GitHub… Click here if not redirected. ",
    "url": "/markdown_files/posts/content/apexcore.html",
    
    "relUrl": "/markdown_files/posts/content/apexcore.html"
  },"48": {
    "doc": "how cancer cells grow",
    "title": "Notes on how are cancer cells grow and spread through the body",
    "content": ". ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#notes-on-how-are-cancer-cells-grow-and-spread-through-the-body",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#notes-on-how-are-cancer-cells-grow-and-spread-through-the-body"
  },"49": {
    "doc": "how cancer cells grow",
    "title": "What are cancer cells?",
    "content": ". | fuck ups in DNA causes genes to mutate, which makes a particular types of cells to not listen to the conditions for which the genes to mutate. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#what-are-cancer-cells",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#what-are-cancer-cells"
  },"50": {
    "doc": "how cancer cells grow",
    "title": "How does Cancer grow?",
    "content": ". | A cancer cell is different from normal ceels because they: . | divide out of control | are immature and don’t develop into doing their jobs | avoid the immune system. | ignore signals | don’t stick to each other very well and can spread to other parts of the body through the blood or lymphatic system | grow into and damage tissues and organs ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-does-cancer-grow",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-does-cancer-grow"
  },"51": {
    "doc": "how cancer cells grow",
    "title": "How cancer spreads?",
    "content": "| . | Normal spread usually by pushing on normal tissues and growing into a big ball in the same place, this is called local invasion or invasive cancer. | Cancer can also spread from where it first started to other parts of the body. This process is called metastatis. Cancer cells can metastatsize when they break away from the tumour and travel to a new location in the body through the blood or lymphatic system. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-cancer-spreads",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-cancer-spreads"
  },"52": {
    "doc": "how cancer cells grow",
    "title": "Why does cancer come back sometimes?",
    "content": ". | even if 1 cancer cell is left begind during cancer treatment, it has the possibility of growing and dividing into a new tumour. | In some cases, treatment may stop working (become resistant) so cancer cells are no longer being destroyed. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#why-does-cancer-come-back-sometimes",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#why-does-cancer-come-back-sometimes"
  },"53": {
    "doc": "how cancer cells grow",
    "title": "Why use acoustic tweezers?",
    "content": ". | It produces high and low acoustic pressures, enabling the movement or positioning of target cells with precision. This allows them to be used for manipulating metastasis-related cancer cells. | The flow cytometry results prove that it is non-invasive. | It can be used to isolate circulating tumor cells (CTCs), and enables in developing targeted treatments. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#why-use-acoustic-tweezers",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#why-use-acoustic-tweezers"
  },"54": {
    "doc": "how cancer cells grow",
    "title": "how to make acoustic tweezers?",
    "content": ". | The new device works by placing sound-creating transducers on each side of a small square chamber filled with liquid. These four transducers work in step with those directly across from them, forming two pairs. One creates patterns in the chamber horizontally and the other vertically. The interaction of these two complex, quickly changing sound wave patterns creates dynamic abilities never before demonstrated within the field. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-to-make-acoustic-tweezers",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#how-to-make-acoustic-tweezers"
  },"55": {
    "doc": "how cancer cells grow",
    "title": "what is acoustic streaming?",
    "content": ". | Acoustic streaming is another important principle that underlies acoustic tweezers. When a sound wave passes through a fluid medium, it creates tiny vortexes of fluid around the edges of the wave. These vortexes can be used to move small objects within the fluid, even against the direction of the sound wave itself. | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#what-is-acoustic-streaming",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#what-is-acoustic-streaming"
  },"56": {
    "doc": "how cancer cells grow",
    "title": "Use cases other than biomedicine",
    "content": ". | In microelectronics, acoustic tweezers have the potential to revolutionize the manufacturing process for microchips and other tiny electronic components. By using sound waves to position and manipulate these components, researchers can improve the accuracy and efficiency of the manufacturing process. | ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#use-cases-other-than-biomedicine",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#use-cases-other-than-biomedicine"
  },"57": {
    "doc": "how cancer cells grow",
    "title": "resources",
    "content": "| How cancer starts, grows and spreads | acoustic tweezers | Cells Dancing to Harmonic Duets Could Enable Personalized Cancer Therapies | . ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html#resources",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html#resources"
  },"58": {
    "doc": "how cancer cells grow",
    "title": "how cancer cells grow",
    "content": " ",
    "url": "/markdown_files/posts/content/basics_of_cancer_cells.html",
    
    "relUrl": "/markdown_files/posts/content/basics_of_cancer_cells.html"
  },"59": {
    "doc": "Blog",
    "title": "Blog",
    "content": " ",
    "url": "/markdown_files/blog.html",
    
    "relUrl": "/markdown_files/blog.html"
  },"60": {
    "doc": "Computer Architecture",
    "title": "Computer Architecture",
    "content": " ",
    "url": "/markdown_files/comparch.html",
    
    "relUrl": "/markdown_files/comparch.html"
  },"61": {
    "doc": "Agents for Microprocessor Evolution",
    "title": "Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution",
    "content": "by yale patt . ",
    "url": "/markdown_files/posts/content/comparchreads1.html#requirements-bottlenecks-and-good-fortune-agents-for-microprocessor-evolution",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html#requirements-bottlenecks-and-good-fortune-agents-for-microprocessor-evolution"
  },"62": {
    "doc": "Agents for Microprocessor Evolution",
    "title": "Basic Framework",
    "content": ". | Computer Architecture: A science of tradeoffs . | Comp arch is more “art” than “science” | Almost always the job of the comp architect requires using that fundamental knowledge to make tradeoffs. | . | Levels of Transformation . | There are levels of transformations that is being done whenever there’s a high level program that is being done. | Like take for example a C code, it first compiles into an assembly code (a lot more process that happens but not imp here). | This “assembly” code is basically converted into where ISA structure is used, where ISA defines on how the assembly code is being defined. | Now, Even though the “assembly” code is now converted into something in binary, how this binary is being processed is (which bits need to go where in order for the “assembly” line to be processed actually) [Instruction is being processed] | This is called microarchitecture. | This microarchitecture is defined by actual electronic circuit design, and in the end electrons. | These are the levels of transformation that occurs from any problem to actually making it solve by a circuit (or by electrons). | . | . | Design Points . | When working in the microarchitecture part, there is always a purpose towards what you are trying to achieve. | This is called the Design Point. | It could be something like, making a core more power efficient , fault tolerant (server chips), highest power etc. | . | Application Space . | Sometimes the chips that we design are application specific, and sometimes generalistic in nature, this is what we call application space here. | . | The Basics of Processing . | Simply put, a microprocessor processes instructions. | To do this, it has to do three things: . | Supply instructions to the core of the processor where the instructions are executed. | Supply data required for the instructions | Perform the operation required by each instruction. | . | . | Instruction Supply . | fetching one instruction at a time is slow when processors are getting faster and better at processing them. | So, the fetching of instructions should be done in parallel, making it more efficient. | . | Data Supply . | Once the instruction is fetched, the data required for the instruction is supplied to the processor. | This must be done in a fast manner as well. | So faster caches (on-chip RAM) are used to supply the data for a particular instruction. | . | Instruction Processing . | Once the instruction is fetched and the data is supplied, the instruction must also be processed fast. | To perform the operations required by these instructions, the processor needs a sufficient number of functional units to process the data. | . | . ",
    "url": "/markdown_files/posts/content/comparchreads1.html#basic-framework",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html#basic-framework"
  },"63": {
    "doc": "Agents for Microprocessor Evolution",
    "title": "Agents for Evolution",
    "content": ". | The creativity of engineers to come up with answers where there were problems— without solutions, there would be no Evolution. | Agent I: New Requirements . | The demand for higher performance dictated that fetching one instruction at a time was not enough. | Examples like this forces evolution and creative solution thinking. | . | Agent II: Bottlenecks . | We have 3 components of instruction processing (instruction supply, data supply, and carrying out the operations of the instruction). | By far, most of the improvements to the microprocessor have come about due to attempts to eliminate bottlenecks that prevent these components from being fully utilized. | . | Agent III: Good Fortune . | Good Fortune happens when something causes a windfall which can then be used to provide additional features to the microprocessor. | . | . ",
    "url": "/markdown_files/posts/content/comparchreads1.html#agents-for-evolution",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html#agents-for-evolution"
  },"64": {
    "doc": "Agents for Microprocessor Evolution",
    "title": "References",
    "content": ". | Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution | . ",
    "url": "/markdown_files/posts/content/comparchreads1.html#references",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html#references"
  },"65": {
    "doc": "Agents for Microprocessor Evolution",
    "title": "Agents for Microprocessor Evolution",
    "content": " ",
    "url": "/markdown_files/posts/content/comparchreads1.html",
    
    "relUrl": "/markdown_files/posts/content/comparchreads1.html"
  },"66": {
    "doc": "Email me!",
    "title": "Email me!",
    "content": "Redirecting to Email… Click here if not redirected. ",
    "url": "/markdown_files/email.html",
    
    "relUrl": "/markdown_files/email.html"
  },"67": {
    "doc": "GitHub",
    "title": "GitHub",
    "content": "Redirecting to GitHub… Click here if not redirected. ",
    "url": "/markdown_files/github.html",
    
    "relUrl": "/markdown_files/github.html"
  },"68": {
    "doc": "How do you get into Computer Architecture",
    "title": "How do you get into Computer Architecture?",
    "content": " ",
    "url": "/markdown_files/posts/content/intro_to_compArch.html#how-do-you-get-into-computer-architecture",
    
    "relUrl": "/markdown_files/posts/content/intro_to_compArch.html#how-do-you-get-into-computer-architecture"
  },"69": {
    "doc": "How do you get into Computer Architecture",
    "title": "How do you get into Computer Architecture",
    "content": " ",
    "url": "/markdown_files/posts/content/intro_to_compArch.html",
    
    "relUrl": "/markdown_files/posts/content/intro_to_compArch.html"
  },"70": {
    "doc": "Kinematics Demo",
    "title": "Kinematics Demo",
    "content": "Redirecting to the Kinematics Demo… Click here if not redirected. ",
    "url": "/kinematics-demo/",
    
    "relUrl": "/kinematics-demo/"
  },"71": {
    "doc": "LinkedIn",
    "title": "LinkedIn",
    "content": "Redirecting to the LinkedIn… Click here if not redirected. ",
    "url": "/markdown_files/linkedin.html",
    
    "relUrl": "/markdown_files/linkedin.html"
  },"72": {
    "doc": "Jee Rank Predictor",
    "title": "Jee Rank Predictor",
    "content": "Redirecting to GitHub… Click here if not redirected. ",
    "url": "/markdown_files/posts/content/mark2jee.html",
    
    "relUrl": "/markdown_files/posts/content/mark2jee.html"
  },"73": {
    "doc": "Math",
    "title": "Math",
    "content": " ",
    "url": "/markdown_files/math.html",
    
    "relUrl": "/markdown_files/math.html"
  },"74": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems",
    "content": "Thomas Moscibroda Onur Mutlu @microsoft . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#memory-performance-attacks-denial-of-memory-service-in-multi-core-systems",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#memory-performance-attacks-denial-of-memory-service-in-multi-core-systems"
  },"75": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Introduction",
    "content": ". | The transition from single-core to multi-core systems has introduced major performance and security challenges. | Multiple programs running on shared DRAM systems can interfere with each other’s memory accesses, leading to performance degradation and security vulnerabilities. | This paper introduces a new security problem that arises due to the core design of multi-core architectures – a denial of service (DoS) attack that was not possible in single-core systems. | An aggressive memory-intensive program can severely impact the performance of other threads with which it is co-scheduled. This is called a Memory Performance Hog (MPH). | This problem worsens with an increasing number of cores, as the impact grows exponentially. | An MPH can be used to perform DoS attacks that fool users into thinking other applications are inherently slow, even without causing easily observable performance issues. | A regular application can unintentionally behave like an MPH and damage the memory-related performance of co-scheduled threads. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#introduction",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#introduction"
  },"76": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "DRAM Architectures",
    "content": ". | DRAM memory is an expensive resource in modern systems. Creating a separate DRAM system for each core is not feasible. | In a partitioned DRAM system, a processor accessing a memory location needs to issue a request to the DRAM partition that contains the data for that location. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#dram-architectures",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#dram-architectures"
  },"77": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "DRAM Memory Systems",
    "content": ". | Row Hit: Accessing a row already in the row-buffer. It has the lowest latency (around 40-50 ns in commodity DRAM). | Row Conflict: Accessing a different row than the one currently in the row-buffer, requiring the row-buffer to be written back before the new row can be accessed. | Row Closed: No row in the row-buffer, necessitating a read from the memory array before column access. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#dram-memory-systems",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#dram-memory-systems"
  },"78": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "DRAM Controller",
    "content": ". | The DRAM controller mediates between on-chip caches and off-chip DRAM memory. It receives read/write requests from L2 caches. | The memory access scheduler is responsible for selecting memory requests from the memory request buffer to send to the DRAM memory. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#dram-controller",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#dram-controller"
  },"79": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Memory Access Scheduling Algorithm",
    "content": ". | Current memory access schedulers typically employ the First-Ready First-Come-First-Serve (FR-FCFS) algorithm, which prioritizes requests in the following order: . | Row-hit first: Prioritizes requests that hit the row already in the row-buffer. | Oldest-within-bank first: Prioritizes requests that arrived earliest within the same bank. | Oldest-across-banks first: Prioritizes the earliest arrival time among requests selected by individual bank schedulers. | . | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#memory-access-scheduling-algorithm",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#memory-access-scheduling-algorithm"
  },"80": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Vulnerabilities of Multi-Core DRAM Memory System to DoS Attacks",
    "content": ". | Current DRAM memory systems do not distinguish between the requests of different threads. | Unfairness of Row-Hit First Scheduling: A thread whose accesses result in row hits gets higher priority compared to a thread whose accesses result in row conflicts. | Unfairness of Oldest-First Scheduling: Oldest-first scheduling implicitly favors threads that can generate memory requests at a faster rate than others. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#vulnerabilities-of-multi-core-dram-memory-system-to-dos-attacks",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#vulnerabilities-of-multi-core-dram-memory-system-to-dos-attacks"
  },"81": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Examples of DoS in Existing Multi-Cores",
    "content": ". | When two threads use different access patterns, such as one streaming data and the other accessing memory randomly, the memory controller will prioritize the one with the optimized memory access pattern. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#examples-of-dos-in-existing-multi-cores",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#examples-of-dos-in-existing-multi-cores"
  },"82": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Fairness in DRAM Memory Systems",
    "content": ". | Defining fairness in DRAM systems is complex, and coming up with a reasonable definition is challenging. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#fairness-in-dram-memory-systems",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#fairness-in-dram-memory-systems"
  },"83": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Fair Memory Scheduling: A Model",
    "content": ". | The authors propose a model for fair memory scheduling to mitigate the impact of MPHs. | Fairness Definition: A memory scheduler is fair if equal-priority threads experience the same memory-related slowdowns when running together. | Stall-Time Fair Memory Scheduler (STFM): STFM prioritizes threads based on their stall times, ensuring that no thread monopolizes memory resources, thus promoting fairness. | Implementation Considerations: STFM requires modifications to the memory controller to track stall times for each thread, ensuring equitable memory access for co-scheduled threads. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#fair-memory-scheduling-a-model",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#fair-memory-scheduling-a-model"
  },"84": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Conclusion",
    "content": ". | The paper highlights the vulnerabilities of multi-core systems to DoS attacks due to unfair memory access scheduling. | By introducing the concept of Memory Performance Hogs and the Stall-Time Fair Memory Scheduler, the authors offer a framework to enhance fairness and improve both the performance and security of multi-core systems. | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#conclusion",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#conclusion"
  },"85": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "References",
    "content": ". | The paper is written by Professor Onur Mutlu which can be found here | . ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html#references",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html#references"
  },"86": {
    "doc": "Memory Performance Attacts and DOS Attacks",
    "title": "Memory Performance Attacts and DOS Attacks",
    "content": " ",
    "url": "/markdown_files/posts/content/memory_performance_attacks.html",
    
    "relUrl": "/markdown_files/posts/content/memory_performance_attacks.html"
  },"87": {
    "doc": "Notes",
    "title": "Notes",
    "content": " ",
    "url": "/markdown_files/notes_stuff.html",
    
    "relUrl": "/markdown_files/notes_stuff.html"
  },"88": {
    "doc": "Other",
    "title": "Other",
    "content": " ",
    "url": "/markdown_files/other.html",
    
    "relUrl": "/markdown_files/other.html"
  },"89": {
    "doc": "Projects",
    "title": "Projects",
    "content": " ",
    "url": "/markdown_files/projects.html",
    
    "relUrl": "/markdown_files/projects.html"
  },"90": {
    "doc": "C-level RISC-V Simulator",
    "title": "C-level RISC-V Simulator",
    "content": "Redirecting to GitHub… Click here if not redirected. ",
    "url": "/markdown_files/posts/content/riscv-in-c.html",
    
    "relUrl": "/markdown_files/posts/content/riscv-in-c.html"
  },"91": {
    "doc": "X",
    "title": "X",
    "content": "Redirecting to X.com… Click here if not redirected. ",
    "url": "/markdown_files/twitter.html",
    
    "relUrl": "/markdown_files/twitter.html"
  }
}
